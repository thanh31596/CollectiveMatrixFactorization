{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from itertools import product, permutations\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "scaler_minmax=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/thanh/OneDrive - Queensland University of Technology/dataPHD/Movie_DePaul/depaul_full.csv')\n",
    "origin_data = pd.read_csv('C:/Users/thanh/OneDrive - Queensland University of Technology/dataPHD/Movie_DePaul/Movie_DePaulMovie/ratings.txt',sep=',')\n",
    "origin_data=origin_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import product\n",
    "\n",
    "# def grid_search(model, param_grid, n_iter=300):\n",
    "#     best_loss = float('inf')\n",
    "#     best_params = None\n",
    "#     for params in product(*param_grid.values()):\n",
    "#         params = dict(zip(param_grid.keys(), params))\n",
    "#         mf = model(**params)\n",
    "#         mf.factorize(iter=n_iter)\n",
    "#         loss = mf.total_loss[-1]\n",
    "#         if loss < best_loss:\n",
    "#             best_loss = loss\n",
    "#             best_params = params\n",
    "#     print('Best parameters:', best_params)\n",
    "#     print('Best loss:', best_loss)\n",
    "#     return best_params\n",
    "\n",
    "\n",
    "# def grid_search(model, param_grid, n_iter=200, filename='grid_search_results.xlsx'):\n",
    "#     best_loss = float('inf')\n",
    "#     best_params = None\n",
    "#     results = []\n",
    "#     for params in product(*param_grid.values()):\n",
    "#         params = dict(zip(param_grid.keys(), params))\n",
    "#         mf = model(**params)\n",
    "#         mf.factorize(iter=n_iter)\n",
    "#         loss = mf.total_loss[-1]\n",
    "#         results.append({**params, 'loss': loss})\n",
    "#         if loss < best_loss:\n",
    "#             best_loss = loss\n",
    "#             best_params = params\n",
    "#     print('Best parameters:', best_params)\n",
    "#     print('Best loss:', best_loss)\n",
    "#     df = pd.DataFrame(results)\n",
    "#     df.to_excel(filename, index=False)\n",
    "#     return best_params\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from multiprocessing import Process, Queue, cpu_count\n",
    "\n",
    "def factorize_worker(model, params_list, n_iter, result_queue):\n",
    "    for params in params_list:\n",
    "        mf = model(**params)\n",
    "        mf.factorize(iter=n_iter)\n",
    "        loss = mf.total_loss[-1]\n",
    "        result_queue.put({**params, 'loss': loss})\n",
    "\n",
    "def grid_search(model, param_grid, n_iter=2, n_jobs=2, filename='grid_search_results.xlsx'):\n",
    "    if n_jobs == -1:\n",
    "        n_jobs = cpu_count()\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_params = None\n",
    "    results = []\n",
    "\n",
    "    params_list = [dict(zip(param_grid.keys(), p)) for p in product(*param_grid.values())]\n",
    "    result_queue = Queue()\n",
    "\n",
    "    processes = []\n",
    "    chunk_size = (len(params_list) + n_jobs - 1) // n_jobs\n",
    "\n",
    "    for i in range(n_jobs):\n",
    "        start = i * chunk_size\n",
    "        end = min((i + 1) * chunk_size, len(params_list))\n",
    "        p = Process(target=factorize_worker, args=(model, params_list[start:end], n_iter, result_queue))\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.start()\n",
    "\n",
    "    for _ in range(len(params_list)):\n",
    "        results.append(result_queue.get())\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_excel(filename, index=False)\n",
    "\n",
    "    best_params = df.loc[df['loss'].idxmin()].to_dict()\n",
    "    best_loss = best_params.pop('loss')\n",
    "\n",
    "    print('Best parameters:', best_params)\n",
    "    print('Best loss:', best_loss)\n",
    "\n",
    "    return best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9896907216494846\n"
     ]
    }
   ],
   "source": [
    "def cal(df):\n",
    "\n",
    "    \n",
    "    # calculate total number of possible user-item interactions\n",
    "    num_users = df[df.columns[0]].nunique()\n",
    "    num_items = df[df.columns[1]].nunique()\n",
    "    num_possible_interactions = num_users * num_items\n",
    "    \n",
    "    # calculate total number of actual user-item interactions\n",
    "    num_actual_interactions = df.shape[0]\n",
    "    \n",
    "    # calculate sparsity of ratings\n",
    "    sparsity = 1 - (num_actual_interactions / num_possible_interactions)\n",
    "    \n",
    "    print(sparsity)\n",
    "cal(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def count_nan(df):\n",
    "    \"\"\"\n",
    "    Returns the percentage of NaN values in a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    total_cells = df.size\n",
    "    nan_cells = df.isna().sum().sum()\n",
    "    nan_percentage = (nan_cells / total_cells) * 100\n",
    "    print(nan_percentage)\n",
    "count_nan(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neighbors=int(df['userid'].value_counts().mean())\n",
    "# imputer = KNNImputer(n_neighbors=neighbors)\n",
    "# imputed_df = pd.DataFrame(imputer.fit_transform(df),columns=['userid', 'itemid', 'rating', 'Time', 'Location', 'Companion'])\n",
    "\n",
    "# for i in imputed_df.columns: \n",
    "#     imputed_df[i] = imputed_df[i].astype('int')\n",
    "# imputed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Contextual Coeficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1:\n",
      "  Time: -0.03644480086388906\n",
      "  Location: 0.04445898358981916\n",
      "  Companion: 0.007458815637433842\n",
      "  userid: -0.03793074542413886\n",
      "Class 2:\n",
      "  Time: 0.0029715428144286462\n",
      "  Location: 0.0037089047383605596\n",
      "  Companion: 0.018126418597485955\n",
      "  userid: -0.008562142412762345\n",
      "Class 3:\n",
      "  Time: 0.02852751061575676\n",
      "  Location: -0.012986803100925636\n",
      "  Companion: 0.03911430835413398\n",
      "  userid: 0.025611039854247532\n",
      "Class 4:\n",
      "  Time: 0.01866442254916665\n",
      "  Location: -0.04726913927029312\n",
      "  Companion: -0.032031569050386476\n",
      "  userid: 0.0011410322853588355\n",
      "Class 5:\n",
      "  Time: -0.01371984984473449\n",
      "  Location: 0.01208380167477715\n",
      "  Companion: -0.03267268445873636\n",
      "  userid: 0.01974224619788895\n",
      "Class 1:\n",
      "  Time: -0.04005891712473817\n",
      "  Location: 0.04843787764028565\n",
      "  Companion: 0.0069900777894147895\n",
      "  itemid: 0.07188273046838618\n",
      "Class 2:\n",
      "  Time: 0.0018659380027313463\n",
      "  Location: 0.004621083145157861\n",
      "  Companion: 0.018001974936739332\n",
      "  itemid: 0.02859474987286544\n",
      "Class 3:\n",
      "  Time: 0.02939077277980255\n",
      "  Location: -0.015608990795321413\n",
      "  Companion: 0.03933166694868651\n",
      "  itemid: 0.01841115644431585\n",
      "Class 4:\n",
      "  Time: 0.018987571615634122\n",
      "  Location: -0.04739835475772306\n",
      "  Companion: -0.03200307877779776\n",
      "  itemid: -0.01135118052236716\n",
      "Class 5:\n",
      "  Time: -0.010189021244284375\n",
      "  Location: 0.009949376249975288\n",
      "  Companion: -0.032319565492151994\n",
      "  itemid: -0.10753740006733653\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming your dataframe is called df\n",
    "X = df[['Time', 'Location', 'Companion','userid']]\n",
    "y = df['rating']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit the LinearSVC model\n",
    "svm = LinearSVC(random_state=42)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = svm.coef_\n",
    "\n",
    "# Print the feature importances\n",
    "features = ['Time', 'Location', 'Companion','userid']\n",
    "for i in range(importances.shape[0]):\n",
    "    print(f'Class {i+1}:')\n",
    "    for j in range(importances.shape[1]):\n",
    "        print(f'  {features[j]}: {importances[i, j]}')\n",
    "\n",
    "# Calculate the mean importance for each feature across all classes\n",
    "mean_importances = np.mean(importances, axis=0)\n",
    "\n",
    "# Create a dictionary to map feature names to mean importances\n",
    "feature_importance_dict = {feature: importance for feature, importance in zip(features, mean_importances)}\n",
    "\n",
    "# Replace the values in 'Time', 'Location', and 'Companion' columns with their respective feature importances\n",
    "df_replaced =df.copy()\n",
    "for feature in features:\n",
    "    df_replaced[feature] = df_replaced[feature] * feature_importance_dict[feature]\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# X = imputed_df[['Time', 'Location', 'Companion']]\n",
    "# y = imputed_df['rating']\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Standardize the features\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Fit the AdaBoost model with a DecisionTreeClassifier as the base estimator\n",
    "# base_estimator = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "# ada = AdaBoostClassifier(base_estimator=base_estimator, random_state=42)\n",
    "# ada.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Get feature importances\n",
    "# importances = ada.feature_importances_\n",
    "\n",
    "# # Print the feature importances\n",
    "# features = ['Time', 'Location', 'Companion']\n",
    "# for i, importance in enumerate(importances):\n",
    "#     print(f'{features[i]}: {importance}')\n",
    "\n",
    "X = df[['Time', 'Location', 'Companion','itemid']]\n",
    "y = df['rating']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit the LinearSVC model\n",
    "svm = LinearSVC(random_state=42)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = svm.coef_\n",
    "\n",
    "# Print the feature importances\n",
    "features = ['Time', 'Location', 'Companion','itemid']\n",
    "for i in range(importances.shape[0]):\n",
    "    print(f'Class {i+1}:')\n",
    "    for j in range(importances.shape[1]):\n",
    "        print(f'  {features[j]}: {importances[i, j]}')\n",
    "\n",
    "# Calculate the mean importance for each feature across all classes\n",
    "mean_importances = np.mean(importances, axis=0)\n",
    "\n",
    "# Create a dictionary to map feature names to mean importances\n",
    "feature_importance_dict = {feature: importance for feature, importance in zip(features, mean_importances)}\n",
    "\n",
    "# Replace the values in 'Time', 'Location', and 'Companion' columns with their respective feature importances\n",
    "df_replaced_item =df.copy()\n",
    "for feature in features:\n",
    "    df_replaced_item[feature] = df_replaced_item[feature] * feature_importance_dict[feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>itemid</th>\n",
       "      <th>Time</th>\n",
       "      <th>Location</th>\n",
       "      <th>Companion</th>\n",
       "      <th>rating</th>\n",
       "      <th>Time_user</th>\n",
       "      <th>Location_user</th>\n",
       "      <th>Companion_user</th>\n",
       "      <th>Time_item</th>\n",
       "      <th>Location_item</th>\n",
       "      <th>Companion_item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5043.000000</td>\n",
       "      <td>5043.000000</td>\n",
       "      <td>5043.000000</td>\n",
       "      <td>5043.000000</td>\n",
       "      <td>5043.000000</td>\n",
       "      <td>5043.000000</td>\n",
       "      <td>5.043000e+03</td>\n",
       "      <td>5.043000e+03</td>\n",
       "      <td>5.043000e+03</td>\n",
       "      <td>5.043000e+03</td>\n",
       "      <td>5.043000e+03</td>\n",
       "      <td>5.043000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1058.351775</td>\n",
       "      <td>36.651398</td>\n",
       "      <td>1.365854</td>\n",
       "      <td>1.323617</td>\n",
       "      <td>1.960539</td>\n",
       "      <td>3.330954</td>\n",
       "      <td>-2.534415e-07</td>\n",
       "      <td>-8.815042e-07</td>\n",
       "      <td>-1.576659e-06</td>\n",
       "      <td>-7.887560e-07</td>\n",
       "      <td>2.055316e-07</td>\n",
       "      <td>3.599184e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>31.922649</td>\n",
       "      <td>23.221948</td>\n",
       "      <td>0.481716</td>\n",
       "      <td>0.467902</td>\n",
       "      <td>0.999320</td>\n",
       "      <td>1.413978</td>\n",
       "      <td>1.889694e-07</td>\n",
       "      <td>6.639891e-07</td>\n",
       "      <td>1.248199e-06</td>\n",
       "      <td>5.881071e-07</td>\n",
       "      <td>1.548157e-07</td>\n",
       "      <td>2.849379e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1001.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4.698917e-07</td>\n",
       "      <td>-1.700947e-06</td>\n",
       "      <td>-2.826552e-06</td>\n",
       "      <td>-1.462388e-06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1034.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-4.698917e-07</td>\n",
       "      <td>-1.700947e-06</td>\n",
       "      <td>-2.826552e-06</td>\n",
       "      <td>-1.462388e-06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1057.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-2.349459e-07</td>\n",
       "      <td>-8.504737e-07</td>\n",
       "      <td>-9.421840e-07</td>\n",
       "      <td>-7.311942e-07</td>\n",
       "      <td>1.982965e-07</td>\n",
       "      <td>2.150810e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1079.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.965929e-07</td>\n",
       "      <td>6.452429e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1123.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.965929e-07</td>\n",
       "      <td>6.452429e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            userid       itemid         Time     Location    Companion  \\\n",
       "count  5043.000000  5043.000000  5043.000000  5043.000000  5043.000000   \n",
       "mean   1058.351775    36.651398     1.365854     1.323617     1.960539   \n",
       "std      31.922649    23.221948     0.481716     0.467902     0.999320   \n",
       "min    1001.000000     0.000000     1.000000     1.000000     1.000000   \n",
       "25%    1034.000000    16.000000     1.000000     1.000000     1.000000   \n",
       "50%    1057.000000    37.000000     1.000000     1.000000     1.000000   \n",
       "75%    1079.000000    55.000000     2.000000     2.000000     3.000000   \n",
       "max    1123.000000    78.000000     2.000000     2.000000     3.000000   \n",
       "\n",
       "            rating     Time_user  Location_user  Companion_user     Time_item  \\\n",
       "count  5043.000000  5.043000e+03   5.043000e+03    5.043000e+03  5.043000e+03   \n",
       "mean      3.330954 -2.534415e-07  -8.815042e-07   -1.576659e-06 -7.887560e-07   \n",
       "std       1.413978  1.889694e-07   6.639891e-07    1.248199e-06  5.881071e-07   \n",
       "min       1.000000 -4.698917e-07  -1.700947e-06   -2.826552e-06 -1.462388e-06   \n",
       "25%       2.000000 -4.698917e-07  -1.700947e-06   -2.826552e-06 -1.462388e-06   \n",
       "50%       4.000000 -2.349459e-07  -8.504737e-07   -9.421840e-07 -7.311942e-07   \n",
       "75%       5.000000  0.000000e+00   0.000000e+00    0.000000e+00  0.000000e+00   \n",
       "max       5.000000  0.000000e+00   0.000000e+00    0.000000e+00  0.000000e+00   \n",
       "\n",
       "       Location_item  Companion_item  \n",
       "count   5.043000e+03    5.043000e+03  \n",
       "mean    2.055316e-07    3.599184e-07  \n",
       "std     1.548157e-07    2.849379e-07  \n",
       "min     0.000000e+00    0.000000e+00  \n",
       "25%     0.000000e+00    0.000000e+00  \n",
       "50%     1.982965e-07    2.150810e-07  \n",
       "75%     3.965929e-07    6.452429e-07  \n",
       "max     3.965929e-07    6.452429e-07  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fix_user = df_replaced[['Time','Location','Companion']]\n",
    "for i in ['Time','Location','Companion']:\n",
    "    for j in range(len(df_fix_user[i])):\n",
    "        if origin_data[i][j] ==0: \n",
    "            df_replaced[i][j]=0\n",
    "df_fix_user = df_replaced_item[['Time','Location','Companion']]\n",
    "for i in ['Time','Location','Companion']:\n",
    "    for j in range(len(df_fix_user[i])):\n",
    "        if origin_data[i][j] ==0: \n",
    "            df_replaced_item[i][j]=0      \n",
    "df['Time_user'] = df_replaced['Time']\n",
    "df['Location_user']= df_replaced['Location']\n",
    "df['Companion_user']= df_replaced['Companion']\n",
    "df['Time_item'] = df_replaced_item['Time']\n",
    "df['Location_item']= df_replaced_item['Location']\n",
    "df['Companion_item']= df_replaced_item['Companion']\n",
    "try:\n",
    "    df=df.drop(columns=['Unnamed: 0'])\n",
    "except KeyError:\n",
    "    pass\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>itemid</th>\n",
       "      <th>Time</th>\n",
       "      <th>Location</th>\n",
       "      <th>Companion</th>\n",
       "      <th>rating</th>\n",
       "      <th>Time_user</th>\n",
       "      <th>Location_user</th>\n",
       "      <th>Companion_user</th>\n",
       "      <th>Time_item</th>\n",
       "      <th>Location_item</th>\n",
       "      <th>Companion_item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1123</td>\n",
       "      <td>58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1123</td>\n",
       "      <td>33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1123</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1123</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1123</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5038</th>\n",
       "      <td>1082</td>\n",
       "      <td>35</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.698917e-07</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>3.965929e-07</td>\n",
       "      <td>6.452429e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5039</th>\n",
       "      <td>1082</td>\n",
       "      <td>62</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>-4.698917e-07</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>3.965929e-07</td>\n",
       "      <td>6.452429e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5040</th>\n",
       "      <td>1082</td>\n",
       "      <td>25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.698917e-07</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>3.965929e-07</td>\n",
       "      <td>6.452429e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5041</th>\n",
       "      <td>1082</td>\n",
       "      <td>50</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.698917e-07</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>3.965929e-07</td>\n",
       "      <td>6.452429e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5042</th>\n",
       "      <td>1082</td>\n",
       "      <td>49</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.698917e-07</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>3.965929e-07</td>\n",
       "      <td>6.452429e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5043 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      userid  itemid  Time  Location  Companion  rating     Time_user  \\\n",
       "0       1123      58   1.0       1.0        1.0       2  0.000000e+00   \n",
       "1       1123      33   1.0       1.0        1.0       4  0.000000e+00   \n",
       "2       1123       1   1.0       1.0        1.0       5  0.000000e+00   \n",
       "3       1123       0   1.0       1.0        1.0       3  0.000000e+00   \n",
       "4       1123      10   1.0       1.0        1.0       3  0.000000e+00   \n",
       "...      ...     ...   ...       ...        ...     ...           ...   \n",
       "5038    1082      35   2.0       2.0        3.0       1 -4.698917e-07   \n",
       "5039    1082      62   2.0       2.0        3.0       2 -4.698917e-07   \n",
       "5040    1082      25   2.0       2.0        3.0       1 -4.698917e-07   \n",
       "5041    1082      50   2.0       2.0        3.0       1 -4.698917e-07   \n",
       "5042    1082      49   2.0       2.0        3.0       1 -4.698917e-07   \n",
       "\n",
       "      Location_user  Companion_user  Time_item  Location_item  Companion_item  \n",
       "0          0.000000        0.000000   0.000000   0.000000e+00    0.000000e+00  \n",
       "1          0.000000        0.000000   0.000000   0.000000e+00    0.000000e+00  \n",
       "2          0.000000        0.000000   0.000000   0.000000e+00    0.000000e+00  \n",
       "3          0.000000        0.000000   0.000000   0.000000e+00    0.000000e+00  \n",
       "4          0.000000        0.000000   0.000000   0.000000e+00    0.000000e+00  \n",
       "...             ...             ...        ...            ...             ...  \n",
       "5038      -0.000002       -0.000003  -0.000001   3.965929e-07    6.452429e-07  \n",
       "5039      -0.000002       -0.000003  -0.000001   3.965929e-07    6.452429e-07  \n",
       "5040      -0.000002       -0.000003  -0.000001   3.965929e-07    6.452429e-07  \n",
       "5041      -0.000002       -0.000003  -0.000001   3.965929e-07    6.452429e-07  \n",
       "5042      -0.000002       -0.000003  -0.000001   3.965929e-07    6.452429e-07  \n",
       "\n",
       "[5043 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.8309537973428514\n",
      "1.6690462026571486\n",
      "0.02786225581874779\n",
      "SUM:  2.7026388144185356\n",
      "-1.9309537973428514\n",
      "1.0023795359904817\n",
      "-0.15725596886280643\n",
      "SUM:  -12.423221540161707\n"
     ]
    }
   ],
   "source": [
    "df_replaced.head(30)\n",
    "global_mean = df_replaced[\"rating\"].mean()\n",
    "user_bias = df_replaced.groupby(\"userid\")[\"rating\"].mean() - global_mean\n",
    "item_bias = df_replaced.groupby(\"itemid\")[\"rating\"].mean() - global_mean\n",
    "print(np.min(user_bias))\n",
    "print(np.max(user_bias))\n",
    "print(np.mean(user_bias))\n",
    "print(\"SUM: \",np.sum(user_bias))\n",
    "print(np.min(item_bias))\n",
    "print(np.max(item_bias))\n",
    "print(np.mean(item_bias))\n",
    "print(\"SUM: \",np.sum(item_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 0, 0, ..., 0, 0, 0],\n",
       "       [4, 4, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [4, 0, 4, ..., 4, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [3, 5, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rating_matrix_original= df[['userid','itemid','rating']].pivot_table(values='rating',index='userid',columns='itemid', fill_value=0).astype('int')\n",
    "rating_matrix_original.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>itemid</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "itemid  0   1   2   3   4   5   6   7   8   9   ...  69  70  71  72  73  74  \\\n",
       "userid                                          ...                           \n",
       "1001     4   0   0   0   3   0   0   1   0   0  ...   0   0   0   0   0   0   \n",
       "1002     4   4   0   0   0   0   3   4   0   0  ...   0   0   0   0   0   2   \n",
       "1003     1   1   1   0   0   1   0   2   2   0  ...   0   0   0   0   0   1   \n",
       "1004     5   0   0   4   4   0   3   3   0   4  ...   0   0   0   3   1   3   \n",
       "1005     2   0   0   0   0   0   0   0   4   0  ...   0   0   0   4   0   4   \n",
       "...     ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..  ..  ..   \n",
       "1117     0   0   4   0   0   0   0   5   0   0  ...   0   0   3   4   0   0   \n",
       "1119     0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   5   \n",
       "1120     4   0   4   0   0   0   0   0   0   3  ...   0   0   0   0   1   4   \n",
       "1122     0   0   0   0   0   0   0   4   0   0  ...   0   0   0   0   0   0   \n",
       "1123     3   5   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   \n",
       "\n",
       "itemid  75  76  77  78  \n",
       "userid                  \n",
       "1001     0   0   0   0  \n",
       "1002     0   0   0   0  \n",
       "1003     0   0   0   0  \n",
       "1004     0   0   3   3  \n",
       "1005     0   0   0   0  \n",
       "...     ..  ..  ..  ..  \n",
       "1117     0   0   0   0  \n",
       "1119     0   0   0   0  \n",
       "1120     0   4   0   0  \n",
       "1122     4   0   0   0  \n",
       "1123     0   0   0   0  \n",
       "\n",
       "[97 rows x 79 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def to_int(x):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    return int(x)\n",
    "rating_matrix=rating_matrix_original\n",
    "rating_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.14623883418100891\n",
      "0.06452429345243926\n",
      "0.0\n",
      "SUM:  -112.6132240790551\n",
      "-0.28265520414363926\n",
      "0.0\n",
      "-0.046989170859740303\n",
      "SUM:  -1367.4622120922938\n"
     ]
    }
   ],
   "source": [
    "def adjacency_matrix_similarity(matrix):\n",
    "    similarity_matrix = matrix @ matrix.T\n",
    "    np.fill_diagonal(similarity_matrix, 0)\n",
    "    return similarity_matrix\n",
    "\n",
    "def degree_matrix(adj_matrix):\n",
    "    degree_vector = np.sum(adj_matrix, axis=1)\n",
    "    return np.diag(degree_vector)\n",
    "\n",
    "def laplacian_matrix(adj_matrix):\n",
    "    deg_matrix = degree_matrix(adj_matrix)\n",
    "    return deg_matrix - adj_matrix\n",
    "neighbors=52\n",
    "\n",
    "# Compute adjacency matrices based on similarity\n",
    "# user_adj_matrix = adjacency_matrix_similarity(U_matrix)\n",
    "# item_adj_matrix = adjacency_matrix_similarity(V_matrix)\n",
    "# # Compute Laplacian matrices\n",
    "# L_U = laplacian_matrix(user_adj_matrix)\n",
    "# L_V = laplacian_matrix(item_adj_matrix)\n",
    "# Get context matrix\n",
    "scaler = StandardScaler()\n",
    "# minmax = MinMaxScaler(feature_range=(0,2))\n",
    "Cu=df[['Time_user',\t'Location_user',\t'Companion_user']].multiply(100000)\n",
    "Ci=df[['Time_item',\t'Location_item',\t'Companion_item']].multiply(100000)\n",
    "C_umatrix =  np.array(Cu)\n",
    "C_imatrix =  np.array(Ci)\n",
    "print(np.min(C_imatrix))\n",
    "print(np.max(C_imatrix))\n",
    "print(np.median(C_imatrix))\n",
    "print(\"SUM: \",np.sum(C_imatrix))\n",
    "print(np.min(C_umatrix))\n",
    "print(np.max(C_umatrix))\n",
    "print(np.median(C_umatrix))\n",
    "print(\"SUM: \",np.sum(C_umatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predicted_ratings, real_ratings):\n",
    "    # Create a mask of the same shape as real_ratings with True where there's a rating and False where there's NaN\n",
    "    mask = ~np.isnan(real_ratings)\n",
    "\n",
    "    # Calculate the squared error between the predicted and real ratings only for the rated items\n",
    "    squared_error = (predicted_ratings[mask] - real_ratings[mask])**2\n",
    "\n",
    "    # Calculate the mean squared error\n",
    "    mean_squared_error = np.mean(squared_error)\n",
    "\n",
    "    # Calculate the root mean squared error\n",
    "    root_mean_squared_error = np.sqrt(mean_squared_error)\n",
    "\n",
    "    return root_mean_squared_error\n",
    "\n",
    "def mae(predicted_ratings, real_ratings):\n",
    "    # Create a mask of the same shape as real_ratings with True where there's a rating and False where there's NaN\n",
    "    mask = ~np.isnan(real_ratings)\n",
    "\n",
    "    # Calculate the absolute error between the predicted and real ratings only for the rated items\n",
    "    absolute_error = np.abs(predicted_ratings[mask] - real_ratings[mask])\n",
    "\n",
    "    # Calculate the mean absolute error\n",
    "    mean_absolute_error = np.mean(absolute_error)\n",
    "\n",
    "    return mean_absolute_error\n",
    "def top_10_f1_score(predicted_ratings, real_ratings):\n",
    "    f1_sum = 0\n",
    "    users_count = real_ratings.shape[0]\n",
    "\n",
    "    for user_idx in range(users_count):\n",
    "        user_real_ratings = real_ratings[user_idx]\n",
    "        user_predicted_ratings = predicted_ratings[user_idx]\n",
    "\n",
    "        # Get the indices of the top-10 predicted ratings\n",
    "        top_10_predicted_indices = np.argsort(user_predicted_ratings)[-10:]\n",
    "\n",
    "        # Get the indices of the user's real ratings\n",
    "        real_rated_indices = np.where(~np.isnan(user_real_ratings))[0]\n",
    "\n",
    "        # Calculate the number of relevant items in the top-10 predicted items\n",
    "        relevant_items_count = np.sum(np.isin(top_10_predicted_indices, real_rated_indices))\n",
    "\n",
    "        # Calculate the precision for the current user\n",
    "        user_precision = relevant_items_count / 10\n",
    "\n",
    "        # Calculate the recall for the current user\n",
    "        user_recall = relevant_items_count / len(real_rated_indices)\n",
    "\n",
    "        # Calculate the F1-score for the current user\n",
    "        if user_precision + user_recall > 0:\n",
    "            user_f1_score = 2 * user_precision * user_recall / (user_precision + user_recall)\n",
    "        else:\n",
    "            user_f1_score = 0\n",
    "\n",
    "        # Update the F1-score sum\n",
    "        f1_sum += user_f1_score\n",
    "\n",
    "    # Calculate the average F1-score across all users\n",
    "    average_f1_score = f1_sum / users_count\n",
    "\n",
    "    return average_f1_score\n",
    "def top_10_precision(predicted_ratings, real_ratings):\n",
    "    precision_sum = 0\n",
    "    users_count = real_ratings.shape[0]\n",
    "\n",
    "    for user_idx in range(users_count):\n",
    "        user_real_ratings = real_ratings[user_idx]\n",
    "        user_predicted_ratings = predicted_ratings[user_idx]\n",
    "\n",
    "        # Get the indices of the top-10 predicted ratings\n",
    "        top_10_predicted_indices = np.argsort(user_predicted_ratings)[-10:]\n",
    "\n",
    "        # Get the indices of the user's real ratings\n",
    "        real_rated_indices = np.where(~np.isnan(user_real_ratings))[0]\n",
    "\n",
    "        # Calculate the number of relevant items in the top-10 predicted items\n",
    "        relevant_items_count = np.sum(np.isin(top_10_predicted_indices, real_rated_indices))\n",
    "\n",
    "        # Calculate the precision for the current user\n",
    "        user_precision = relevant_items_count / 10\n",
    "\n",
    "        # Update the precision sum\n",
    "        precision_sum += user_precision\n",
    "\n",
    "    # Calculate the average precision across all users\n",
    "    average_precision = precision_sum / users_count\n",
    "\n",
    "    return average_precision\n",
    "\n",
    "def top_1_precision(predicted_ratings, real_ratings):\n",
    "    precision_sum = 0\n",
    "    users_count = real_ratings.shape[0]\n",
    "\n",
    "    for user_idx in range(users_count):\n",
    "        user_real_ratings = real_ratings[user_idx]\n",
    "        user_predicted_ratings = predicted_ratings[user_idx]\n",
    "\n",
    "        # Get the indices of the top-10 predicted ratings\n",
    "        top_10_predicted_indices = np.argsort(user_predicted_ratings)[-1:]\n",
    "\n",
    "        # Get the indices of the user's real ratings\n",
    "        real_rated_indices = np.where(~np.isnan(user_real_ratings))[0]\n",
    "\n",
    "        # Calculate the number of relevant items in the top-10 predicted items\n",
    "        relevant_items_count = np.sum(np.isin(top_10_predicted_indices, real_rated_indices))\n",
    "\n",
    "        # Calculate the precision for the current user\n",
    "        user_precision = relevant_items_count \n",
    "\n",
    "        # Update the precision sum\n",
    "        precision_sum += user_precision\n",
    "\n",
    "    # Calculate the average precision across all users\n",
    "    average_precision = precision_sum / users_count\n",
    "\n",
    "    return average_precision\n",
    "def precision_at_k(predicted_ratings, real_ratings, k=10):\n",
    "    precision_sum = 0\n",
    "    users_count = real_ratings.shape[0]\n",
    "\n",
    "    for user_idx in range(users_count):\n",
    "        user_real_ratings = real_ratings[user_idx]\n",
    "        user_predicted_ratings = predicted_ratings[user_idx]\n",
    "\n",
    "        # Get the indices of the top-k predicted ratings\n",
    "        top_k_predicted_indices = np.argsort(user_predicted_ratings)[-k:]\n",
    "\n",
    "        # Get the indices of the user's real ratings\n",
    "        real_rated_indices = np.where(~np.isnan(user_real_ratings))[0]\n",
    "\n",
    "        # Calculate the number of relevant items in the top-k predicted items\n",
    "        relevant_items_count = np.sum(np.isin(top_k_predicted_indices, real_rated_indices))\n",
    "\n",
    "        # Calculate the precision for the current user\n",
    "        user_precision = relevant_items_count / k\n",
    "\n",
    "        # Update the precision sum\n",
    "        precision_sum += user_precision\n",
    "\n",
    "    # Calculate the average precision across all users\n",
    "    average_precision = precision_sum / users_count\n",
    "\n",
    "    return average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def k_fold_cross_validation(ratings, k=5, random_state=None):\n",
    "    # Create a KFold object with the specified number of folds\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Initialize the list to store the train and test matrices for each fold\n",
    "    train_test_matrices = []\n",
    "\n",
    "    # Iterate over the splits\n",
    "    for train_indices, test_indices in kf.split(ratings):\n",
    "        # Copy the original ratings matrix for both train and test matrices\n",
    "        train_matrix = ratings.copy()\n",
    "        test_matrix = np.empty_like(ratings)\n",
    "\n",
    "        test_matrix[:] = np.nan\n",
    "\n",
    "        # Replace the test ratings with NaN in the train matrix and vice versa\n",
    "        for row, col in zip(*np.nonzero(ratings)):\n",
    "            if row in test_indices:\n",
    "                train_matrix[row, col] = np.nan\n",
    "                test_matrix[row, col] = ratings[row, col]\n",
    "\n",
    "        # Add the train and test matrices to the list\n",
    "        train_test_matrices.append((train_matrix, test_matrix))\n",
    "\n",
    "    return train_test_matrices\n",
    "\n",
    "class MultiMF: \n",
    "    def __init__(self,R_, P_, Q_,U_,V_,  E_, D_, C_u_,C_i_,  L_U_, L_V_, lambdas_, b_u_, b_v_,alpha):\n",
    "\n",
    "        self.R_matrix = np.array(R_)\n",
    "        self.P_matrix = np.array(P_) \n",
    "        self.Q_matrix = np.array(Q_) \n",
    "        self.lambdas = lambdas_ \n",
    "        self.U_matrix = np.array(U_)\n",
    "        self.V_matrix = np.array(V_)\n",
    "        self.E_matrix = np.array(E_)\n",
    "        self.D_matrix = np.array(D_) \n",
    "        self.C_umatrix=np.array(C_u_) \n",
    "        self.C_imatrix=np.array(C_i_) \n",
    "        self.L_U=L_U_\n",
    "        self.L_V=L_V_\n",
    "        self.item_bias=np.array(b_v_)\n",
    "        self.user_bias=np.array(b_u_)\n",
    "        self.newU=[]\n",
    "        self.newV=[]\n",
    "        self.newE=[]\n",
    "        self.newD=[]\n",
    "        self.alpha=alpha\n",
    "        self.total_loss=[0]\n",
    "    def factorize(self,iter=10):\n",
    "        self.run_func()\n",
    "        for k in tqdm(range(iter)):\n",
    "            if abs(self.total_loss[-1] - self.total_loss[-2]) < 0.0004:\n",
    "                print(\"Success\")\n",
    "                break\n",
    "            else: \n",
    "                # self.U_matrix=np.array(self.newU)\n",
    "                # self.V_matrix=np.array(self.newV).T\n",
    "                # self.D_matrix=np.array(self.newD)\n",
    "                # self.E_matrix=np.array(self.newE)\n",
    "                # self.newU=[]\n",
    "                # self.newV=[]\n",
    "                # self.newE=[]\n",
    "                # self.newD=[]\n",
    "                self.run_func()\n",
    "        # return self.U_matrix, self.V_matrix\n",
    "    def run_func(self): \n",
    "        loss_col = 0 \n",
    "        lambdas=self.lambdas\n",
    "        for i in range(len(self.R_matrix)):  #70\n",
    "            loss_row = 0\n",
    "            U = self.U_matrix[i]\n",
    "            C_u = self.C_umatrix[i,:]\n",
    "            C_i = self.C_imatrix[i,:]\n",
    "            b_u=self.user_bias[i]\n",
    "            for j in range(len(self.R_matrix[i])): #97\n",
    "\n",
    "                if not np.isnan(self.R_matrix[i,j]):\n",
    "\n",
    "                    R = self.R_matrix[i,j]\n",
    "                    V = self.V_matrix.T[j]\n",
    "                    b_v=self.item_bias[j]\n",
    "                    #First situation\n",
    "                    P=Q=L_U=L_V=E=D=0\n",
    "\n",
    "                    U, V, D, E = self.update_U_V_E_D(R=R, P=0, E=0, D=0, Q=0, U=U, V=V, C_u=C_u, C_i=C_i,Lu=0,Li=0, lambdas=lambdas)\n",
    "                    self.U_matrix[i] = U\n",
    "                    self.V_matrix.T[j] = V\n",
    "                    loss,U_o,V_o,D_o,E_o = self.objective_function(R, P, Q, U, V, E, D, C_u,C_i,  L_U, L_V, lambdas, b_u, b_v)\n",
    "\n",
    "\n",
    "\n",
    "                    loss_row+=loss\n",
    "                    # U_ = self.update_U(R, P, Q, U, V, E, D, C_u, C_i, lambdas, alpha)\n",
    "            loss_col+=loss_row\n",
    "        self.total_loss.append(loss_col)\n",
    "    def objective_function(self,R, P, Q, U, V, E, D, C_u,C_i,  L_U, L_V, lambdas, b_u, b_v):\n",
    "        # U=U+0.0000000000005\n",
    "        # V=V+0.0000000000005\n",
    "        M = U.shape[0] #number of user\n",
    "        V_=0\n",
    "        U_=0\n",
    "        \n",
    "        U_ += np.sum(C_u)*U\n",
    "        V_ += np.sum(C_i)*V\n",
    "        U_mean=U_/M\n",
    "        V_mean=V_/M\n",
    "        R_pred = np.dot(U_mean + U, V.T + V_mean)\n",
    "        if P !=0:\n",
    "            P_pred = np.dot(U, E.T)\n",
    "            Q_pred = np.dot(V, D.T)\n",
    "        else:\n",
    "            P_pred = 0\n",
    "            Q_pred = 0\n",
    "        loss = np.sum((R - R_pred)**2) + np.sum((P - P_pred)**2) + np.sum((Q - Q_pred)**2)\n",
    "        loss += lambdas[0] * np.sum(U**2) + lambdas[1] * np.sum(V**2) + lambdas[2] * np.sum(D**2) + lambdas[3] * np.sum(E**2)\n",
    "        loss += b_u + b_v\n",
    "        \n",
    "        return loss,U,V,D,E\n",
    "\n",
    "    def update_U_V_E_D(self,R, P, Q, U, V, E, D, C_u,C_i,Lu,Li,lambdas):\n",
    "        alpha=self.alpha\n",
    "        # U=U+0.0000000000005\n",
    "        # V=V+0.0000000000005\n",
    "        M = U.shape[0] #number of user\n",
    "        V_=0\n",
    "        U_=0\n",
    "        U_ += np.sum(C_u)*U\n",
    "        V_ += np.sum(C_i)*V\n",
    "        U_mean=U_/M\n",
    "\n",
    "        V_mean=V_/M\n",
    "        if P != 0:\n",
    "            P_pred = np.dot(U, E.T)\n",
    "            Q_pred = np.dot(V, D.T)\n",
    "        else:\n",
    "            P_pred = 0\n",
    "            Q_pred = 0\n",
    "        R_pred = np.dot((U_mean + U), (V.T + V_mean))\n",
    "\n",
    "\n",
    "        dU = -2 * (R- R_pred) * (V.T + V_mean)*(1+U_mean) - 2 * (P - P_pred) * E + 2 * lambdas[0] * U   + 2*lambdas[3]*Lu #change to lambda 5 soon\n",
    "        dV = -2 * (R - R_pred) * (U_mean + U)*(1+V_mean) - 2 * (Q - Q_pred) * D + 2 * lambdas[1] * V  + 2*lambdas[3]*Li\n",
    "        dE = -2 * (P - P_pred) * U + 2 * lambdas[3] * E\n",
    "        dD = -2 * (Q - Q_pred) * V + 2 * lambdas[2] * D\n",
    "        \n",
    "        U -= alpha * dU\n",
    "        V -= alpha * dV\n",
    "        E -= alpha * dE\n",
    "        D -= alpha * dD\n",
    "        return U, V, E, D\n",
    "    def predict(self):\n",
    "        self.factorize()\n",
    "        mmscaler=MinMaxScaler(feature_range=(1, 6))\n",
    "        R_pred = np.dot(self.U_matrix,self.V_matrix)\n",
    "        R_pred=mmscaler.fit_transform(R_pred).astype(int)\n",
    "        return R_pred\n",
    "    def evaluation(self,test):\n",
    "        import datetime\n",
    "\n",
    "        R_pred = self.predict()\n",
    "        print(R_pred)\n",
    "        mae_value = mae(R_pred,test)\n",
    "        rmse_value = rmse(R_pred,test)\n",
    "        top_10_f1_score_value = top_10_f1_score(R_pred,test)\n",
    "        top_10_precision_value = top_10_precision(R_pred,test)\n",
    "        top_1_precision_value = top_1_precision(R_pred,test)\n",
    "        precision_at_k_value = precision_at_k(R_pred,test)\n",
    "        result = pd.DataFrame()\n",
    "        result['RMSE']=[rmse_value]\n",
    "        result['MAE']= [mae_value]\n",
    "        result['top_10_f1_score']= [top_10_f1_score_value]\n",
    "        result['top_10_precision']= [top_10_precision_value]\n",
    "        result['top_1_precision']= [top_1_precision_value]\n",
    "        result['precision_at_k']= [precision_at_k_value]\n",
    "        print(result)\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        filename = f\"output_{timestamp}.csv\"\n",
    "        result.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 0, 0, ..., 0, 0, 0],\n",
       "       [4, 4, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [4, 0, 4, ..., 4, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [3, 5, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_matrix.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "       [ 4.,  4., nan, ..., nan, nan, nan],\n",
       "       [ 1.,  1.,  1., ..., nan, nan, nan],\n",
       "       ...,\n",
       "       [ 4., nan,  4., ...,  4., nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [ 3.,  5., nan, ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "zxz = k_fold_cross_validation(rating_matrix.replace(to_replace=0, value=np.nan).values)\n",
    "train_m=zxz[0][0]\n",
    "test_m =zxz[0][1]\n",
    "train_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "values = [0.001, 0.01, 0.1, 0, 1, 10, 100]\n",
    "values = [0.001,  0.1, 1, 10]\n",
    "combinations = list(itertools.product(values, repeat=4))\n",
    "combinations_arr = np.array(combinations)\n",
    "len(combinations_arr)\n",
    "model = NMF(n_components=10, init='random', random_state=0) #n_component = KNN values\n",
    "U_matrix = model.fit_transform(np.nan_to_num(train_m,0))\n",
    "V_matrix = model.components_\n",
    "# U_matrix[U_matrix == 0] = 0.000005\n",
    "# V_matrix[V_matrix == 0] = 0.000005\n",
    "if (np.isnan(U_matrix ).any() or np.isnan(V_matrix ).any()):\n",
    "    print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'alpha': [1,0.01],\n",
    "              'lambdas_': [0.1],\n",
    "              'R_': [train_m],\n",
    "              'P_': [0],\n",
    "              'U_':[U_matrix],\n",
    "              'V_':[V_matrix],\n",
    "              'Q_': [0],\n",
    "              'E_': [0],\n",
    "              'D_': [0],\n",
    "              'C_u_': [C_umatrix],\n",
    "              'C_i_': [C_imatrix],\n",
    "              'L_U_': [0],\n",
    "              'L_V_': [0],\n",
    "              'b_u_': [user_bias.values],\n",
    "              'b_v_': [item_bias.values]}\n",
    "best_params = grid_search(MultiMF, param_grid)\n",
    "# alpha = best_params['alpha']\n",
    "# lambdas = best_params['lambdas_']\n",
    "# k = best_params['k']\n",
    "# object = MultiMF(rating_matrix.values,0,0,0,0,C_umatrix,C_imatrix,0,0,lambdas,user_bias.values,item_bias.values,alpha)\n",
    "# U_result, V_result=object.factorize(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 1 1 1]\n",
      " [3 3 2 ... 1 1 1]\n",
      " [2 1 1 ... 1 1 1]\n",
      " ...\n",
      " [4 1 2 ... 3 1 2]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [2 2 1 ... 1 1 1]]\n",
      "      RMSE       MAE  top_10_f1_score  top_10_precision  top_1_precision  \\\n",
      "0  2.54951  2.142857         0.017277          0.020619         0.030928   \n",
      "\n",
      "   precision_at_k  \n",
      "0        0.020619  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "object = MultiMF(train_m,0,0,U_matrix,V_matrix,0,0,C_umatrix,C_imatrix,0,0,[0.1,0.01,0.1,0.001],user_bias.values,item_bias.values,0.001)\n",
    "# object.factorize(5)\n",
    "object.evaluation(test_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.16666667 0.5        0.66666667 1.        ]\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "# define the vector to be scaled\n",
    "vector = [2, 3, 5, 6, 8]\n",
    "\n",
    "# reshape the vector to a 2D array to fit the input requirements of MinMaxScaler\n",
    "vector = np.array(vector).reshape(-1, 1)\n",
    "\n",
    "# fit the scaler to the data and transform the data using the scaler\n",
    "scaled_vector = scaler.fit_transform(vector)\n",
    "\n",
    "# print the scaled vector\n",
    "print(scaled_vector.T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchhere.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd26f88697b863e66c6ece3fc4ab8542eed3593bce93b36f41fad6131c5cf6bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
