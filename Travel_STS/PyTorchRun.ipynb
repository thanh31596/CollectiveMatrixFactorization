{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thanh\\OneDrive - Queensland University of Technology\\dataPHD\\pytorchhere.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from itertools import product, permutations\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "scaler_minmax=MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('travel_full.csv').drop(columns=['Unnamed: 0'])\n",
    "origin_data=  pd.read_csv('Data_STS.csv', sep='\\t')\n",
    "\n",
    "origin_data=origin_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import product\n",
    "\n",
    "# def grid_search(model, param_grid, n_iter=300):\n",
    "#     best_loss = float('inf')\n",
    "#     best_params = None\n",
    "#     for params in product(*param_grid.values()):\n",
    "#         params = dict(zip(param_grid.keys(), params))\n",
    "#         mf = model(**params)\n",
    "#         mf.factorize(iter=n_iter)\n",
    "#         loss = mf.total_loss[-1]\n",
    "#         if loss < best_loss:\n",
    "#             best_loss = loss\n",
    "#             best_params = params\n",
    "#     print('Best parameters:', best_params)\n",
    "#     print('Best loss:', best_loss)\n",
    "#     return best_params\n",
    "\n",
    "\n",
    "def grid_search(model, param_grid, test, filename='travel_grid_search_results.xlsx'):\n",
    "    best_loss = float('inf')\n",
    "    best_params = None\n",
    "    results = []\n",
    "    for params in product(*param_grid.values()):\n",
    "        params = dict(zip(param_grid.keys(), params))\n",
    "        mf = model(**params)\n",
    "        mf.evaluation(test)\n",
    "        loss = mf.total_loss[-1]\n",
    "        results.append({**params, 'loss': loss})\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_params = params\n",
    "    print('Best parameters:', best_params)\n",
    "    print('Best loss:', best_loss)\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_excel(filename, index=False)\n",
    "    return best_params\n",
    "# import pandas as pd\n",
    "# from itertools import product\n",
    "# from multiprocessing import Process, Queue, cpu_count\n",
    "\n",
    "# def factorize_worker(model, params_list, n_iter, result_queue):\n",
    "#     for params in params_list:\n",
    "#         mf = model(**params)\n",
    "#         mf.factorize(iter=n_iter)\n",
    "#         loss = mf.total_loss[-1]\n",
    "#         result_queue.put({**params, 'loss': loss})\n",
    "\n",
    "# def grid_search(model, param_grid, n_iter=2, n_jobs=2, filename='grid_search_results.xlsx'):\n",
    "#     if n_jobs == -1:\n",
    "#         n_jobs = cpu_count()\n",
    "\n",
    "#     best_loss = float('inf')\n",
    "#     best_params = None\n",
    "#     results = []\n",
    "\n",
    "#     params_list = [dict(zip(param_grid.keys(), p)) for p in product(*param_grid.values())]\n",
    "#     result_queue = Queue()\n",
    "\n",
    "#     processes = []\n",
    "#     chunk_size = (len(params_list) + n_jobs - 1) // n_jobs\n",
    "\n",
    "#     for i in range(n_jobs):\n",
    "#         start = i * chunk_size\n",
    "#         end = min((i + 1) * chunk_size, len(params_list))\n",
    "#         p = Process(target=factorize_worker, args=(model, params_list[start:end], n_iter, result_queue))\n",
    "#         processes.append(p)\n",
    "\n",
    "#     for p in processes:\n",
    "#         p.start()\n",
    "\n",
    "#     for _ in range(len(params_list)):\n",
    "#         results.append(result_queue.get())\n",
    "\n",
    "#     for p in processes:\n",
    "#         p.join()\n",
    "\n",
    "#     df = pd.DataFrame(results)\n",
    "#     df.to_excel(filename, index=False)\n",
    "\n",
    "#     best_params = df.loc[df['loss'].idxmin()].to_dict()\n",
    "#     best_loss = best_params.pop('loss')\n",
    "\n",
    "#     print('Best parameters:', best_params)\n",
    "#     print('Best loss:', best_loss)\n",
    "\n",
    "#     return best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9686870559159716\n"
     ]
    }
   ],
   "source": [
    "def cal(df):\n",
    "\n",
    "    \n",
    "    # calculate total number of possible user-item interactions\n",
    "    num_users = df[df.columns[0]].nunique()\n",
    "    num_items = df[df.columns[1]].nunique()\n",
    "    num_possible_interactions = num_users * num_items\n",
    "    \n",
    "    # calculate total number of actual user-item interactions\n",
    "    num_actual_interactions = df.shape[0]\n",
    "    \n",
    "    # calculate sparsity of ratings\n",
    "    sparsity = 1 - (num_actual_interactions / num_possible_interactions)\n",
    "    \n",
    "    print(sparsity)\n",
    "cal(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def count_nan(df):\n",
    "    \"\"\"\n",
    "    Returns the percentage of NaN values in a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    total_cells = df.size\n",
    "    nan_cells = df.isna().sum().sum()\n",
    "    nan_percentage = (nan_cells / total_cells) * 100\n",
    "    print(nan_percentage)\n",
    "count_nan(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['userID', 'itemID', 'gender', 'opennessToExperience',\n",
       "       'conscientiousness', 'extraversion', 'agreeableness',\n",
       "       'emotionalStability', 'distance', 'timeAvailable', 'temperature',\n",
       "       'crowdedness', 'knowledgeOfSurroundings', 'season', 'budget', 'daytime',\n",
       "       'weather', 'companion', 'mood', 'weekday', 'travelGoal', 'transport',\n",
       "       'category1', 'category2', 'category3', 'rating'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# neighbors=int(df['userid'].value_counts().mean())\n",
    "# imputer = KNNImputer(n_neighbors=neighbors)\n",
    "# imputed_df = pd.DataFrame(imputer.fit_transform(df),columns=['userid', 'itemid', 'rating', 'Time', 'Location', 'Companion'])\n",
    "\n",
    "# for i in imputed_df.columns: \n",
    "#     imputed_df[i] = imputed_df[i].astype('int')\n",
    "# imputed_df\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Contextual Coeficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1:\n",
      "  userID: 0.02884661846573517\n",
      "  gender: 0.003748811687425532\n",
      "  opennessToExperience: -0.055352408491916\n",
      "  conscientiousness: -0.04829555751191756\n",
      "  extraversion: 0.026083182133697518\n",
      "  agreeableness: 0.009183346696952761\n",
      "  emotionalStability: -0.00353426397902396\n",
      "  distance: -0.047368426675396515\n",
      "  timeAvailable: -0.010222539374455632\n",
      "  temperature: -0.04131110773194485\n",
      "  crowdedness: -0.008299592126735946\n",
      "  knowledgeOfSurroundings: -0.02216760628387529\n",
      "  season: -0.03461790820442387\n",
      "  budget: 0.0052499439981564654\n",
      "  daytime: 0.011517031553850973\n",
      "  weather: -0.02038698536042554\n",
      "  companion: -0.029770750549493467\n",
      "  mood: 0.016799667799987726\n",
      "  weekday: -0.05630799728282222\n",
      "  travelGoal: 0.012882472230471618\n",
      "  transport: -0.027840901330798376\n",
      "  category1: -0.08238176611357301\n",
      "  category2: -0.011605018295691898\n",
      "  category3: 0.006645169667302016\n",
      "Class 2:\n",
      "  userID: 0.0017839902946475727\n",
      "  gender: -0.013382979676117736\n",
      "  opennessToExperience: -0.017755159090501394\n",
      "  conscientiousness: -0.03971253505149097\n",
      "  extraversion: -0.012519297250603746\n",
      "  agreeableness: -0.017740877606839726\n",
      "  emotionalStability: 0.03504326435369555\n",
      "  distance: -0.02101900799292245\n",
      "  timeAvailable: -0.029418461956981164\n",
      "  temperature: -0.021933143752818667\n",
      "  crowdedness: 0.013618873127661062\n",
      "  knowledgeOfSurroundings: -0.026010641575882783\n",
      "  season: -0.002554144576113654\n",
      "  budget: -0.012379105604864594\n",
      "  daytime: -0.013610383687205106\n",
      "  weather: 0.024378381487495163\n",
      "  companion: -0.012061324723665496\n",
      "  mood: -0.010066366556169137\n",
      "  weekday: -0.004009808977479931\n",
      "  travelGoal: 0.0005605556511090648\n",
      "  transport: -0.010518561588259224\n",
      "  category1: -0.03707989480563351\n",
      "  category2: -0.018849598983703204\n",
      "  category3: -0.017557189625615727\n",
      "Class 3:\n",
      "  userID: 0.010785114942163743\n",
      "  gender: -0.017740226253367505\n",
      "  opennessToExperience: 0.013312945313144657\n",
      "  conscientiousness: 0.004540435386774942\n",
      "  extraversion: 0.029324014279739883\n",
      "  agreeableness: 0.006839116937479508\n",
      "  emotionalStability: -0.06440229265477578\n",
      "  distance: 0.01927794209351131\n",
      "  timeAvailable: 0.004447930059534994\n",
      "  temperature: -0.012723695285883777\n",
      "  crowdedness: 0.02449867232577273\n",
      "  knowledgeOfSurroundings: -0.009522516528935149\n",
      "  season: -0.020210545925447725\n",
      "  budget: 0.0050244515593619275\n",
      "  daytime: -0.01748865343307835\n",
      "  weather: -0.010602380222060346\n",
      "  companion: -0.005542152796719953\n",
      "  mood: -0.008987849621569594\n",
      "  weekday: -0.002192852256375862\n",
      "  travelGoal: -0.000767349755590064\n",
      "  transport: -0.00429534509503897\n",
      "  category1: -0.017785035782579287\n",
      "  category2: 0.010673323087979997\n",
      "  category3: -0.0004551063509472291\n",
      "Class 4:\n",
      "  userID: -0.0029066742010079856\n",
      "  gender: 0.026387358003371386\n",
      "  opennessToExperience: 0.002811412925628267\n",
      "  conscientiousness: 0.10186767319721957\n",
      "  extraversion: -0.02310607109055044\n",
      "  agreeableness: -0.07088620325374405\n",
      "  emotionalStability: 0.027198512107550442\n",
      "  distance: 0.03005069579313114\n",
      "  timeAvailable: -0.01575330960002598\n",
      "  temperature: 0.062181997789255795\n",
      "  crowdedness: -0.011744453483177368\n",
      "  knowledgeOfSurroundings: 0.016710011029427523\n",
      "  season: 0.040446218196527856\n",
      "  budget: -0.03771389653553097\n",
      "  daytime: 0.04102377501656318\n",
      "  weather: -0.006161479524243833\n",
      "  companion: 0.05054608021287177\n",
      "  mood: -0.002429116161328683\n",
      "  weekday: 0.015178018251649066\n",
      "  travelGoal: 0.010191360668777091\n",
      "  transport: 0.048971038176254936\n",
      "  category1: -0.004579160999954734\n",
      "  category2: 0.012931235825152517\n",
      "  category3: 0.017272200934876126\n",
      "Class 5:\n",
      "  userID: -0.03514818326096593\n",
      "  gender: 0.0032163038876170096\n",
      "  opennessToExperience: 0.05506720074638133\n",
      "  conscientiousness: -0.01743814595461994\n",
      "  extraversion: -0.016883334846744796\n",
      "  agreeableness: 0.07208048433047132\n",
      "  emotionalStability: 0.006704040053297245\n",
      "  distance: 0.010655776622910555\n",
      "  timeAvailable: 0.05014819535888689\n",
      "  temperature: 0.008685408570459671\n",
      "  crowdedness: -0.018803731238857004\n",
      "  knowledgeOfSurroundings: 0.038735341098608586\n",
      "  season: 0.009742538633558945\n",
      "  budget: 0.03982291869929715\n",
      "  daytime: -0.01972567254461747\n",
      "  weather: 0.01067787224156754\n",
      "  companion: -0.008113601011114245\n",
      "  mood: 0.006265832545555784\n",
      "  weekday: 0.03319822315609261\n",
      "  travelGoal: -0.02071957790705517\n",
      "  transport: -0.010616626413369813\n",
      "  category1: 0.11358062506913012\n",
      "  category2: 0.0018882145859881426\n",
      "  category3: -0.00531704145601994\n",
      "Class 1:\n",
      "  itemID: 0.025557833458117303\n",
      "  gender: 0.004062782268750531\n",
      "  opennessToExperience: -0.05588239899318156\n",
      "  conscientiousness: -0.04345501209891098\n",
      "  extraversion: 0.024225841897178798\n",
      "  agreeableness: 0.01593245955695434\n",
      "  emotionalStability: 0.0001943376421824672\n",
      "  distance: -0.047416104069976005\n",
      "  timeAvailable: -0.011594871851456719\n",
      "  temperature: -0.041599021291180434\n",
      "  crowdedness: -0.008125415461064175\n",
      "  knowledgeOfSurroundings: -0.02144400612199168\n",
      "  season: -0.034611089493754084\n",
      "  budget: 0.0045756696661130945\n",
      "  daytime: 0.011096772827200969\n",
      "  weather: -0.0200613700830319\n",
      "  companion: -0.02977877221795208\n",
      "  mood: 0.017363351363772744\n",
      "  weekday: -0.05653450242012324\n",
      "  travelGoal: 0.012303229250656947\n",
      "  transport: -0.027727111672621145\n",
      "  category1: -0.09326057826468179\n",
      "  category2: -0.013021072874866585\n",
      "  category3: 0.0055327660738932905\n",
      "Class 2:\n",
      "  itemID: 0.025984183535599243\n",
      "  gender: -0.014270473104860503\n",
      "  opennessToExperience: -0.018495509648124995\n",
      "  conscientiousness: -0.041471302730613904\n",
      "  extraversion: -0.01730011379855157\n",
      "  agreeableness: -0.01617893093582473\n",
      "  emotionalStability: 0.03425926327504597\n",
      "  distance: -0.020946118141550892\n",
      "  timeAvailable: -0.029810493141042834\n",
      "  temperature: -0.02182421075673322\n",
      "  crowdedness: 0.01319742738921363\n",
      "  knowledgeOfSurroundings: -0.024747502323798166\n",
      "  season: -0.002943383348845352\n",
      "  budget: -0.013187442647608589\n",
      "  daytime: -0.013585743830841962\n",
      "  weather: 0.023853763006971083\n",
      "  companion: -0.012171943524306967\n",
      "  mood: -0.009620749117770885\n",
      "  weekday: -0.003241017558565061\n",
      "  travelGoal: 0.0007193338596430344\n",
      "  transport: -0.00968580465745162\n",
      "  category1: -0.05191109977442175\n",
      "  category2: -0.018630757603834293\n",
      "  category3: -0.017098429657436645\n",
      "Class 3:\n",
      "  itemID: -0.05692904996575274\n",
      "  gender: -0.015441498325396185\n",
      "  opennessToExperience: 0.01512265787055599\n",
      "  conscientiousness: 0.011324776224869663\n",
      "  extraversion: 0.041495276293355185\n",
      "  agreeableness: 0.005979090228250185\n",
      "  emotionalStability: -0.05999111587816712\n",
      "  distance: 0.01878036182713187\n",
      "  timeAvailable: 0.004979096467570653\n",
      "  temperature: -0.014113156699211597\n",
      "  crowdedness: 0.025017900405131435\n",
      "  knowledgeOfSurroundings: -0.011837360004898647\n",
      "  season: -0.018658796689560997\n",
      "  budget: 0.006549361463629759\n",
      "  daytime: -0.017868944666936358\n",
      "  weather: -0.009935053868108456\n",
      "  companion: -0.005323127448581029\n",
      "  mood: -0.009252568395428368\n",
      "  weekday: -0.002914071149601048\n",
      "  travelGoal: -0.0009951728574198676\n",
      "  transport: -0.008410748551510812\n",
      "  category1: 0.017524887404877232\n",
      "  category2: 0.009180489747110852\n",
      "  category3: -0.002294030992914899\n",
      "Class 4:\n",
      "  itemID: 0.05392001094456346\n",
      "  gender: 0.02501751193812583\n",
      "  opennessToExperience: 0.0014219669748486206\n",
      "  conscientiousness: 0.09673974112591156\n",
      "  extraversion: -0.034002133771903975\n",
      "  agreeableness: -0.06834600045049756\n",
      "  emotionalStability: 0.02399559421693029\n",
      "  distance: 0.030423264996008365\n",
      "  timeAvailable: -0.01601958874352616\n",
      "  temperature: 0.062282801134921816\n",
      "  crowdedness: -0.012237552550161266\n",
      "  knowledgeOfSurroundings: 0.01836829638637611\n",
      "  season: 0.03936976744475859\n",
      "  budget: -0.03884430487449112\n",
      "  daytime: 0.0410824461159589\n",
      "  weather: -0.007158293507824985\n",
      "  companion: 0.05056624500981777\n",
      "  mood: -0.0022884958912264687\n",
      "  weekday: 0.015839118415534866\n",
      "  travelGoal: 0.009952766832769156\n",
      "  transport: 0.05248351307619244\n",
      "  category1: -0.03609234576148834\n",
      "  category2: 0.013275175441632482\n",
      "  category3: 0.018688047335180452\n",
      "Class 5:\n",
      "  itemID: -0.0445761455440591\n",
      "  gender: 0.003126162399313397\n",
      "  opennessToExperience: 0.05680343433078884\n",
      "  conscientiousness: -0.021778272505744177\n",
      "  extraversion: -0.01222938043899273\n",
      "  agreeableness: 0.06248970680525983\n",
      "  emotionalStability: 0.0030576047164589506\n",
      "  distance: 0.011564832762961847\n",
      "  timeAvailable: 0.05116128896704686\n",
      "  temperature: 0.00903079509670302\n",
      "  crowdedness: -0.017970992382013346\n",
      "  knowledgeOfSurroundings: 0.03684155291634513\n",
      "  season: 0.010823339298206268\n",
      "  budget: 0.04038774024361326\n",
      "  daytime: -0.01937009472249829\n",
      "  weather: 0.011374830323350106\n",
      "  companion: -0.009025478835677395\n",
      "  mood: 0.006966808835595111\n",
      "  weekday: 0.03263666601767895\n",
      "  travelGoal: -0.02025562781128008\n",
      "  transport: -0.01166869182967907\n",
      "  category1: 0.13494730705344968\n",
      "  category2: 0.0036143727415917353\n",
      "  category3: -0.004406007524825048\n"
     ]
    }
   ],
   "source": [
    "# Assuming your dataframe is called df\n",
    "X = df[['userID', 'gender', 'opennessToExperience',\n",
    "       'conscientiousness', 'extraversion', 'agreeableness',\n",
    "       'emotionalStability', 'distance', 'timeAvailable', 'temperature',\n",
    "       'crowdedness', 'knowledgeOfSurroundings', 'season', 'budget', 'daytime',\n",
    "       'weather', 'companion', 'mood', 'weekday', 'travelGoal', 'transport',\n",
    "       'category1', 'category2', 'category3']]\n",
    "y = df['rating']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit the LinearSVC model\n",
    "svm = LinearSVC(random_state=42)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = svm.coef_\n",
    "\n",
    "# Print the feature importances\n",
    "features = ['userID', 'gender', 'opennessToExperience',\n",
    "       'conscientiousness', 'extraversion', 'agreeableness',\n",
    "       'emotionalStability', 'distance', 'timeAvailable', 'temperature',\n",
    "       'crowdedness', 'knowledgeOfSurroundings', 'season', 'budget', 'daytime',\n",
    "       'weather', 'companion', 'mood', 'weekday', 'travelGoal', 'transport',\n",
    "       'category1', 'category2', 'category3']\n",
    "for i in range(importances.shape[0]):\n",
    "    print(f'Class {i+1}:')\n",
    "    for j in range(importances.shape[1]):\n",
    "        print(f'  {features[j]}: {importances[i, j]}')\n",
    "\n",
    "# Calculate the mean importance for each feature across all classes\n",
    "mean_importances = np.mean(importances, axis=0)\n",
    "\n",
    "# Create a dictionary to map feature names to mean importances\n",
    "feature_importance_dict = {feature: importance for feature, importance in zip(features, mean_importances)}\n",
    "\n",
    "# Replace the values in 'Time', 'Location', and 'Companion' columns with their respective feature importances\n",
    "df_replaced =df.copy()\n",
    "for feature in features:\n",
    "    df_replaced[feature] = df_replaced[feature] * feature_importance_dict[feature]\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# X = imputed_df[['Time', 'Location', 'Companion']]\n",
    "# y = imputed_df['rating']\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Standardize the features\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Fit the AdaBoost model with a DecisionTreeClassifier as the base estimator\n",
    "# base_estimator = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "# ada = AdaBoostClassifier(base_estimator=base_estimator, random_state=42)\n",
    "# ada.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Get feature importances\n",
    "# importances = ada.feature_importances_\n",
    "\n",
    "# # Print the feature importances\n",
    "# features = ['Time', 'Location', 'Companion']\n",
    "# for i, importance in enumerate(importances):\n",
    "#     print(f'{features[i]}: {importance}')\n",
    "\n",
    "X = df[['itemID', 'gender', 'opennessToExperience',\n",
    "       'conscientiousness', 'extraversion', 'agreeableness',\n",
    "       'emotionalStability', 'distance', 'timeAvailable', 'temperature',\n",
    "       'crowdedness', 'knowledgeOfSurroundings', 'season', 'budget', 'daytime',\n",
    "       'weather', 'companion', 'mood', 'weekday', 'travelGoal', 'transport',\n",
    "       'category1', 'category2', 'category3']]\n",
    "y = df['rating']\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit the LinearSVC model\n",
    "svm = LinearSVC(random_state=42)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = svm.coef_\n",
    "\n",
    "# Print the feature importances\n",
    "features = ['itemID', 'gender', 'opennessToExperience',\n",
    "       'conscientiousness', 'extraversion', 'agreeableness',\n",
    "       'emotionalStability', 'distance', 'timeAvailable', 'temperature',\n",
    "       'crowdedness', 'knowledgeOfSurroundings', 'season', 'budget', 'daytime',\n",
    "       'weather', 'companion', 'mood', 'weekday', 'travelGoal', 'transport',\n",
    "       'category1', 'category2', 'category3']\n",
    "for i in range(importances.shape[0]):\n",
    "    print(f'Class {i+1}:')\n",
    "    for j in range(importances.shape[1]):\n",
    "        print(f'  {features[j]}: {importances[i, j]}')\n",
    "\n",
    "# Calculate the mean importance for each feature across all classes\n",
    "mean_importances = np.mean(importances, axis=0)\n",
    "\n",
    "# Create a dictionary to map feature names to mean importances\n",
    "feature_importance_dict = {feature: importance for feature, importance in zip(features, mean_importances)}\n",
    "\n",
    "# Replace the values in 'Time', 'Location', and 'Companion' columns with their respective feature importances\n",
    "df_replaced_item =df.copy()\n",
    "for feature in features:\n",
    "    df_replaced_item[feature] = df_replaced_item[feature] * feature_importance_dict[feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>gender</th>\n",
       "      <th>opennessToExperience</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>emotionalStability</th>\n",
       "      <th>distance</th>\n",
       "      <th>timeAvailable</th>\n",
       "      <th>...</th>\n",
       "      <th>weather</th>\n",
       "      <th>companion</th>\n",
       "      <th>mood</th>\n",
       "      <th>weekday</th>\n",
       "      <th>travelGoal</th>\n",
       "      <th>transport</th>\n",
       "      <th>category1</th>\n",
       "      <th>category2</th>\n",
       "      <th>category3</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2529</th>\n",
       "      <td>324.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2530</th>\n",
       "      <td>324.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2531</th>\n",
       "      <td>324.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>324.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533</th>\n",
       "      <td>325.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2534 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      userID  itemID  gender  opennessToExperience  conscientiousness  \\\n",
       "0        1.0     1.0     1.0                   4.0                4.0   \n",
       "1        2.0     2.0     1.0                   2.0                1.0   \n",
       "2        2.0     2.0     1.0                   4.0                2.0   \n",
       "3        2.0     2.0     1.0                   4.0                2.0   \n",
       "4        2.0     2.0     1.0                   2.0                1.0   \n",
       "...      ...     ...     ...                   ...                ...   \n",
       "2529   324.0    53.0     2.0                   4.0                7.0   \n",
       "2530   324.0    46.0     2.0                   4.0                7.0   \n",
       "2531   324.0    43.0     2.0                   4.0                7.0   \n",
       "2532   324.0    56.0     2.0                   4.0                7.0   \n",
       "2533   325.0    14.0     1.0                   5.0                4.0   \n",
       "\n",
       "      extraversion  agreeableness  emotionalStability  distance  \\\n",
       "0              4.0            4.0                 3.0       1.0   \n",
       "1              1.0            2.0                 2.0       1.0   \n",
       "2              2.0            5.0                 4.0       1.0   \n",
       "3              1.0            3.0                 2.0       1.0   \n",
       "4              1.0            2.0                 2.0       1.0   \n",
       "...            ...            ...                 ...       ...   \n",
       "2529           2.0            3.0                 4.0       1.0   \n",
       "2530           2.0            3.0                 4.0       1.0   \n",
       "2531           2.0            3.0                 4.0       1.0   \n",
       "2532           2.0            3.0                 4.0       1.0   \n",
       "2533           2.0            2.0                 2.0       1.0   \n",
       "\n",
       "      timeAvailable  ...  weather  companion  mood  weekday  travelGoal  \\\n",
       "0               1.0  ...      1.0        1.0   1.0      1.0         1.0   \n",
       "1               1.0  ...      3.0        3.0   2.0      1.0         2.0   \n",
       "2               2.0  ...      1.0        1.0   1.0      1.0         1.0   \n",
       "3               1.0  ...      1.0        1.0   2.0      1.0         3.0   \n",
       "4               1.0  ...      3.0        3.0   1.0      1.0         2.0   \n",
       "...             ...  ...      ...        ...   ...      ...         ...   \n",
       "2529            1.0  ...      1.0        2.0   2.0      1.0         1.0   \n",
       "2530            1.0  ...      3.0        3.0   1.0      1.0         2.0   \n",
       "2531            2.0  ...      3.0        3.0   1.0      1.0         1.0   \n",
       "2532            2.0  ...      1.0        3.0   2.0      1.0         1.0   \n",
       "2533            1.0  ...      1.0        1.0   1.0      1.0         2.0   \n",
       "\n",
       "      transport  category1  category2  category3  rating  \n",
       "0           1.0        1.0       19.0       26.0     4.0  \n",
       "1           1.0        1.0       14.0       26.0     1.0  \n",
       "2           1.0        1.0       15.0       26.0     1.0  \n",
       "3           2.0        1.0       18.0       26.0     1.0  \n",
       "4           1.0        1.0       14.0       26.0     1.0  \n",
       "...         ...        ...        ...        ...     ...  \n",
       "2529        1.0        4.0       19.0       26.0     4.0  \n",
       "2530        1.0        4.0       13.0       26.0     5.0  \n",
       "2531        2.0        4.0       14.0       26.0     3.0  \n",
       "2532        2.0        8.0       15.0       26.0     4.0  \n",
       "2533        1.0        4.0       13.0       26.0     5.0  \n",
       "\n",
       "[2534 rows x 26 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fix_user = df_replaced[['gender', 'opennessToExperience',\n",
    "       'conscientiousness', 'extraversion', 'agreeableness',\n",
    "       'emotionalStability', 'distance', 'timeAvailable', 'temperature',\n",
    "       'crowdedness', 'knowledgeOfSurroundings', 'season', 'budget', 'daytime',\n",
    "       'weather', 'companion', 'mood', 'weekday', 'travelGoal', 'transport',\n",
    "       'category1', 'category2', 'category3']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix_user = df_replaced[['gender', 'opennessToExperience',\n",
    "       'conscientiousness', 'extraversion', 'agreeableness',\n",
    "       'emotionalStability', 'distance', 'timeAvailable', 'temperature',\n",
    "       'crowdedness', 'knowledgeOfSurroundings', 'season', 'budget', 'daytime',\n",
    "       'weather', 'companion', 'mood', 'weekday', 'travelGoal', 'transport',\n",
    "       'category1', 'category2', 'category3']]\n",
    "for i in ['gender', 'opennessToExperience',\n",
    "       'conscientiousness', 'extraversion', 'agreeableness',\n",
    "       'emotionalStability', 'distance', 'timeAvailable', 'temperature',\n",
    "       'crowdedness', 'knowledgeOfSurroundings', 'season', 'budget', 'daytime',\n",
    "       'weather', 'companion', 'mood', 'weekday', 'travelGoal', 'transport',\n",
    "       'category1', 'category2', 'category3']:\n",
    "    for j in range(len(df_fix_user[i])):\n",
    "        if origin_data[i][j] ==0: \n",
    "            df_replaced[i][j]=0\n",
    "df_fix_user = df_replaced_item[['gender', 'opennessToExperience',\n",
    "       'conscientiousness', 'extraversion', 'agreeableness',\n",
    "       'emotionalStability', 'distance', 'timeAvailable', 'temperature',\n",
    "       'crowdedness', 'knowledgeOfSurroundings', 'season', 'budget', 'daytime',\n",
    "       'weather', 'companion', 'mood', 'weekday', 'travelGoal', 'transport',\n",
    "       'category1', 'category2', 'category3']]\n",
    "for i in ['gender', 'opennessToExperience',\n",
    "       'conscientiousness', 'extraversion', 'agreeableness',\n",
    "       'emotionalStability', 'distance', 'timeAvailable', 'temperature',\n",
    "       'crowdedness', 'knowledgeOfSurroundings', 'season', 'budget', 'daytime',\n",
    "       'weather', 'companion', 'mood', 'weekday', 'travelGoal', 'transport',\n",
    "       'category1', 'category2', 'category3']:\n",
    "    for j in range(len(df_fix_user[i])):\n",
    "        if origin_data[i][j] ==0: \n",
    "            df_replaced_item[i][j]=0      \n",
    "for i in df_fix_user.columns: \n",
    "    prefix = \"_user\"\n",
    "    # create a new column name by adding the prefix to the original column name\n",
    "    new_col = i+prefix\n",
    "    # create the new column by copying the original column data to the new column\n",
    "    df[new_col] = df_replaced[i]\n",
    "for i in df_fix_user.columns: \n",
    "    prefix = \"_item\"\n",
    "    # create a new column name by adding the prefix to the original column name\n",
    "    new_col = i+prefix\n",
    "    # create the new column by copying the original column data to the new column\n",
    "    df[new_col] = df_replaced_item[i]\n",
    "try:\n",
    "    df=df.drop(columns=['Unnamed: 0'])\n",
    "except KeyError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['userID', 'itemID', 'gender', 'opennessToExperience',\n",
       "       'conscientiousness', 'extraversion', 'agreeableness',\n",
       "       'emotionalStability', 'distance', 'timeAvailable', 'temperature',\n",
       "       'crowdedness', 'knowledgeOfSurroundings', 'season', 'budget', 'daytime',\n",
       "       'weather', 'companion', 'mood', 'weekday', 'travelGoal', 'transport',\n",
       "       'category1', 'category2', 'category3', 'rating', 'gender_user',\n",
       "       'opennessToExperience_user', 'conscientiousness_user',\n",
       "       'extraversion_user', 'agreeableness_user', 'emotionalStability_user',\n",
       "       'distance_user', 'timeAvailable_user', 'temperature_user',\n",
       "       'crowdedness_user', 'knowledgeOfSurroundings_user', 'season_user',\n",
       "       'budget_user', 'daytime_user', 'weather_user', 'companion_user',\n",
       "       'mood_user', 'weekday_user', 'travelGoal_user', 'transport_user',\n",
       "       'category1_user', 'category2_user', 'category3_user', 'gender_item',\n",
       "       'opennessToExperience_item', 'conscientiousness_item',\n",
       "       'extraversion_item', 'agreeableness_item', 'emotionalStability_item',\n",
       "       'distance_item', 'timeAvailable_item', 'temperature_item',\n",
       "       'crowdedness_item', 'knowledgeOfSurroundings_item', 'season_item',\n",
       "       'budget_item', 'daytime_item', 'weather_item', 'companion_item',\n",
       "       'mood_item', 'weekday_item', 'travelGoal_item', 'transport_item',\n",
       "       'category1_item', 'category2_item', 'category3_item'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.4715864246250985\n",
      "1.5284135753749015\n",
      "0.21361193194488473\n",
      "SUM:  69.42387788208754\n",
      "-2.4715864246250985\n",
      "1.5284135753749015\n",
      "0.16928395559480514\n",
      "SUM:  42.15170494310648\n"
     ]
    }
   ],
   "source": [
    "df_replaced.head(30)\n",
    "global_mean = df_replaced[\"rating\"].mean()\n",
    "user_bias = df_replaced.groupby(\"userID\")[\"rating\"].mean() - global_mean\n",
    "item_bias = df_replaced.groupby(\"itemID\")[\"rating\"].mean() - global_mean\n",
    "print(np.min(user_bias))\n",
    "print(np.max(user_bias))\n",
    "print(np.mean(user_bias))\n",
    "print(\"SUM: \",np.sum(user_bias))\n",
    "print(np.min(item_bias))\n",
    "print(np.max(item_bias))\n",
    "print(np.mean(item_bias))\n",
    "print(\"SUM: \",np.sum(item_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 2, 4, ..., 0, 0, 0],\n",
       "       [1, 1, 2, ..., 0, 0, 0],\n",
       "       [1, 0, 2, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_matrix_original= df[['userID','itemID','rating']].pivot_table(values='rating',index='userID',columns='itemID', fill_value=0).astype('int')\n",
    "rating_matrix_original.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate real Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_int(x):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    return int(x)\n",
    "rating_matrix=rating_matrix_original\n",
    "# df_onehot = pd.get_dummies(df_2[['category_id', ' artist']])\n",
    "# similarity_matrix = cosine_similarity(df_onehot)\n",
    "# def calculate_L(similarity_matrix):\n",
    "#     L = []\n",
    "\n",
    "#     for i in range(len(similarity_matrix)):\n",
    "#         a = 0\n",
    "#         for j in range(len(similarity_matrix)):\n",
    "#             a = similarity_matrix[i][j] * np.sum((similarity_matrix[i] - similarity_matrix[j]))\n",
    "#         L.append(a)\n",
    "#     return L\n",
    "# df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.16699261018513675\n",
      "0.0031041526938645476\n",
      "0.0\n",
      "SUM:  -48.91980299144071\n",
      "-0.16382234926914035\n",
      "0.0040578905157537875\n",
      "0.0\n",
      "SUM:  -50.66990928245317\n"
     ]
    }
   ],
   "source": [
    "neighbors=52\n",
    "\n",
    "# Compute adjacency matrices based on similarity\n",
    "# user_adj_matrix = adjacency_matrix_similarity(U_matrix)\n",
    "# item_adj_matrix = adjacency_matrix_similarity(V_matrix)\n",
    "# # Compute Laplacian matrices\n",
    "# L_U = laplacian_matrix(user_adj_matrix)\n",
    "# L_V = laplacian_matrix(item_adj_matrix)\n",
    "# Get context matrix\n",
    "scaler = StandardScaler()\n",
    "# minmax = MinMaxScaler(feature_range=(0,2))\n",
    "Cu=df[['gender_user', 'opennessToExperience_user', 'conscientiousness_user', 'extraversion_user', 'agreeableness_user', 'emotionalStability_user', 'distance_user', 'timeAvailable_user', 'temperature_user', 'crowdedness_user', 'knowledgeOfSurroundings_user', 'season_user', 'budget_user', 'daytime_user', 'weather_user', 'companion_user', 'mood_user', 'weekday_user', 'travelGoal_user', 'transport_user', 'category1_user', 'category2_user', 'category3_user',]]\n",
    "Ci=df[['gender_item', 'opennessToExperience_item', 'conscientiousness_item', 'extraversion_item', 'agreeableness_item', 'emotionalStability_item', 'distance_item', 'timeAvailable_item', 'temperature_item', 'crowdedness_item', 'knowledgeOfSurroundings_item', 'season_item', 'budget_item', 'daytime_item', 'weather_item', 'companion_item', 'mood_item', 'weekday_item', 'travelGoal_item', 'transport_item', 'category1_item', 'category2_item', 'category3_item']]\n",
    "C_umatrix =  np.array(Cu)\n",
    "C_imatrix =  np.array(Ci)\n",
    "print(np.min(C_imatrix))\n",
    "print(np.max(C_imatrix))\n",
    "print(np.median(C_imatrix))\n",
    "print(\"SUM: \",np.sum(C_imatrix))\n",
    "print(np.min(C_umatrix))\n",
    "print(np.max(C_umatrix))\n",
    "print(np.median(C_umatrix))\n",
    "print(\"SUM: \",np.sum(C_umatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predicted_ratings, real_ratings):\n",
    "    # Create a mask of the same shape as real_ratings with True where there's a rating and False where there's NaN\n",
    "    mask = ~np.isnan(real_ratings)\n",
    "\n",
    "    # Calculate the squared error between the predicted and real ratings only for the rated items\n",
    "    squared_error = (predicted_ratings[mask] - real_ratings[mask])**2\n",
    "\n",
    "    # Calculate the mean squared error\n",
    "    mean_squared_error = np.mean(squared_error)\n",
    "\n",
    "    # Calculate the root mean squared error\n",
    "    root_mean_squared_error = np.sqrt(mean_squared_error)\n",
    "\n",
    "    return root_mean_squared_error\n",
    "\n",
    "def mae(predicted_ratings, real_ratings):\n",
    "    # Create a mask of the same shape as real_ratings with True where there's a rating and False where there's NaN\n",
    "    mask = ~np.isnan(real_ratings)\n",
    "\n",
    "    # Calculate the absolute error between the predicted and real ratings only for the rated items\n",
    "    absolute_error = np.abs(predicted_ratings[mask] - real_ratings[mask])\n",
    "\n",
    "    # Calculate the mean absolute error\n",
    "    mean_absolute_error = np.mean(absolute_error)\n",
    "\n",
    "    return mean_absolute_error\n",
    "def top_10_f1_score(predicted_ratings, real_ratings):\n",
    "    f1_sum = 0\n",
    "    users_count = real_ratings.shape[0]\n",
    "\n",
    "    for user_idx in range(users_count):\n",
    "        user_real_ratings = real_ratings[user_idx]\n",
    "        user_predicted_ratings = predicted_ratings[user_idx]\n",
    "\n",
    "        # Get the indices of the top-10 predicted ratings\n",
    "        top_10_predicted_indices = np.argsort(user_predicted_ratings)[-10:]\n",
    "\n",
    "        # Get the indices of the user's real ratings\n",
    "        real_rated_indices = np.where(~np.isnan(user_real_ratings))[0]\n",
    "\n",
    "        # Calculate the number of relevant items in the top-10 predicted items\n",
    "        relevant_items_count = np.sum(np.isin(top_10_predicted_indices, real_rated_indices))\n",
    "\n",
    "        # Calculate the precision for the current user\n",
    "        user_precision = relevant_items_count / 10\n",
    "\n",
    "        # Calculate the recall for the current user\n",
    "        user_recall = relevant_items_count / len(real_rated_indices)\n",
    "\n",
    "        # Calculate the F1-score for the current user\n",
    "        if user_precision + user_recall > 0:\n",
    "            user_f1_score = 2 * user_precision * user_recall / (user_precision + user_recall)\n",
    "        else:\n",
    "            user_f1_score = 0\n",
    "\n",
    "        # Update the F1-score sum\n",
    "        f1_sum += user_f1_score\n",
    "\n",
    "    # Calculate the average F1-score across all users\n",
    "    average_f1_score = f1_sum / users_count\n",
    "\n",
    "    return average_f1_score\n",
    "def top_10_precision(predicted_ratings, real_ratings):\n",
    "    precision_sum = 0\n",
    "    users_count = real_ratings.shape[0]\n",
    "\n",
    "    for user_idx in range(users_count):\n",
    "        user_real_ratings = real_ratings[user_idx]\n",
    "        user_predicted_ratings = predicted_ratings[user_idx]\n",
    "\n",
    "        # Get the indices of the top-10 predicted ratings\n",
    "        top_10_predicted_indices = np.argsort(user_predicted_ratings)[-10:]\n",
    "\n",
    "        # Get the indices of the user's real ratings\n",
    "        real_rated_indices = np.where(~np.isnan(user_real_ratings))[0]\n",
    "\n",
    "        # Calculate the number of relevant items in the top-10 predicted items\n",
    "        relevant_items_count = np.sum(np.isin(top_10_predicted_indices, real_rated_indices))\n",
    "\n",
    "        # Calculate the precision for the current user\n",
    "        user_precision = relevant_items_count / 10\n",
    "\n",
    "        # Update the precision sum\n",
    "        precision_sum += user_precision\n",
    "\n",
    "    # Calculate the average precision across all users\n",
    "    average_precision = precision_sum / users_count\n",
    "\n",
    "    return average_precision\n",
    "\n",
    "def top_1_precision(predicted_ratings, real_ratings):\n",
    "    precision_sum = 0\n",
    "    users_count = real_ratings.shape[0]\n",
    "\n",
    "    for user_idx in range(users_count):\n",
    "        user_real_ratings = real_ratings[user_idx]\n",
    "        user_predicted_ratings = predicted_ratings[user_idx]\n",
    "\n",
    "        # Get the indices of the top-10 predicted ratings\n",
    "        top_10_predicted_indices = np.argsort(user_predicted_ratings)[-1:]\n",
    "\n",
    "        # Get the indices of the user's real ratings\n",
    "        real_rated_indices = np.where(~np.isnan(user_real_ratings))[0]\n",
    "\n",
    "        # Calculate the number of relevant items in the top-10 predicted items\n",
    "        relevant_items_count = np.sum(np.isin(top_10_predicted_indices, real_rated_indices))\n",
    "\n",
    "        # Calculate the precision for the current user\n",
    "        user_precision = relevant_items_count \n",
    "\n",
    "        # Update the precision sum\n",
    "        precision_sum += user_precision\n",
    "\n",
    "    # Calculate the average precision across all users\n",
    "    average_precision = precision_sum / users_count\n",
    "\n",
    "    return average_precision\n",
    "def precision_at_k(predicted_ratings, real_ratings, k=10):\n",
    "    precision_sum = 0\n",
    "    users_count = real_ratings.shape[0]\n",
    "\n",
    "    for user_idx in range(users_count):\n",
    "        user_real_ratings = real_ratings[user_idx]\n",
    "        user_predicted_ratings = predicted_ratings[user_idx]\n",
    "\n",
    "        # Get the indices of the top-k predicted ratings\n",
    "        top_k_predicted_indices = np.argsort(user_predicted_ratings)[-k:]\n",
    "\n",
    "        # Get the indices of the user's real ratings\n",
    "        real_rated_indices = np.where(~np.isnan(user_real_ratings))[0]\n",
    "\n",
    "        # Calculate the number of relevant items in the top-k predicted items\n",
    "        relevant_items_count = np.sum(np.isin(top_k_predicted_indices, real_rated_indices))\n",
    "\n",
    "        # Calculate the precision for the current user\n",
    "        user_precision = relevant_items_count / k\n",
    "\n",
    "        # Update the precision sum\n",
    "        precision_sum += user_precision\n",
    "\n",
    "    # Calculate the average precision across all users\n",
    "    average_precision = precision_sum / users_count\n",
    "\n",
    "    return average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def k_fold_cross_validation(ratings, k=5, random_state=None):\n",
    "    # Create a KFold object with the specified number of folds\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Initialize the list to store the train and test matrices for each fold\n",
    "    train_test_matrices = []\n",
    "\n",
    "    # Iterate over the splits\n",
    "    for train_indices, test_indices in kf.split(ratings):\n",
    "        # Copy the original ratings matrix for both train and test matrices\n",
    "        train_matrix = ratings.copy()\n",
    "        test_matrix = np.empty_like(ratings)\n",
    "\n",
    "        test_matrix[:] = np.nan\n",
    "\n",
    "        # Replace the test ratings with NaN in the train matrix and vice versa\n",
    "        for row, col in zip(*np.nonzero(ratings)):\n",
    "            if row in test_indices:\n",
    "                train_matrix[row, col] = np.nan\n",
    "                test_matrix[row, col] = ratings[row, col]\n",
    "\n",
    "        # Add the train and test matrices to the list\n",
    "        train_test_matrices.append((train_matrix, test_matrix))\n",
    "\n",
    "    return train_test_matrices\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MultiMF:\n",
    "    def __init__(self, R_, P_, Q_, U_, V_, E_, D_, C_u_, C_i_, L_U_, L_V_, lambdas_, b_u_, b_v_, alpha):\n",
    "        self.R_matrix = torch.tensor(R_, dtype=torch.float32)\n",
    "        self.P_matrix = torch.tensor(P_, dtype=torch.float32)\n",
    "        self.Q_matrix = torch.tensor(Q_, dtype=torch.float32)\n",
    "        self.lambdas = lambdas_\n",
    "        self.U_matrix = torch.tensor(U_, dtype=torch.float32, requires_grad=True)\n",
    "        self.V_matrix = torch.tensor(V_, dtype=torch.float32, requires_grad=True)\n",
    "        self.E_matrix = torch.tensor(E_, dtype=torch.float32, requires_grad=True)\n",
    "        self.D_matrix = torch.tensor(D_, dtype=torch.float32, requires_grad=True)\n",
    "        self.C_umatrix = torch.tensor(C_u_, dtype=torch.float32)\n",
    "        self.C_imatrix = torch.tensor(C_i_, dtype=torch.float32)\n",
    "        self.L_U = torch.tensor(L_U_, dtype=torch.float32)\n",
    "        self.L_V = torch.tensor(L_V_, dtype=torch.float32)\n",
    "        self.item_bias = torch.tensor(b_v_, dtype=torch.float32)\n",
    "        self.user_bias = torch.tensor(b_u_, dtype=torch.float32)\n",
    "        self.alpha = alpha\n",
    "        self.total_loss = [0]\n",
    "\n",
    "    def factorize(self, iter=200):\n",
    "        self.run_func()\n",
    "        for k in tqdm(range(iter)):\n",
    "            if abs(self.total_loss[-1] - self.total_loss[-2]) < 0.0004:\n",
    "                print(\"Success\")\n",
    "                break\n",
    "            else:\n",
    "                self.run_func()\n",
    "\n",
    "    def run_func(self):\n",
    "        loss_col = 0\n",
    "        lambdas = self.lambdas\n",
    "\n",
    "\n",
    "        for i in range(len(self.R_matrix)):  # 70\n",
    "            loss_row = 0\n",
    "            U = self.U_matrix[i]\n",
    "            C_u = self.C_umatrix[i, :]\n",
    "            C_i = self.C_imatrix[i, :]\n",
    "            b_u = self.user_bias[i]\n",
    "            for j in range(len(self.R_matrix[i])):  # 97\n",
    "\n",
    "                if not torch.isnan(self.R_matrix[i, j]):\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    R = self.R_matrix[i, j]\n",
    "                    V = self.V_matrix[:, j]\n",
    "                    # E = self.E_matrix[i]\n",
    "                    # Q = self.Q_matrix[j]\n",
    "                    # P = self.P_matrix[i]\n",
    "                    # D = self.D_matrix[j]\n",
    "                    # Li = self.L_V[j]\n",
    "                    # Lu = self.L_U[i]\n",
    "                    E = 0\n",
    "                    Q = 0\n",
    "                    P = 0\n",
    "                    D = 0\n",
    "                    Li = 0\n",
    "                    Lu = 0\n",
    "                    b_v = self.item_bias[j]\n",
    "\n",
    "                    loss = self.objective_function(R, P, Q, U, V, E, D, C_u, C_i, Lu, Li, lambdas, b_u, b_v)\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    loss_row += loss.item()\n",
    "\n",
    "            loss_col += loss_row\n",
    "\n",
    "        self.total_loss.append(loss_col)\n",
    "\n",
    "    def objective_function(self, R, P, Q, U, V, E, D, C_u, C_i, L_U, L_V, lambdas, b_u, b_v):\n",
    "        M = U.shape[0]  # number of users\n",
    "        V_ = 0\n",
    "        U_ = 0\n",
    "\n",
    "        U_ += torch.sum(C_u) * U\n",
    "        V_ += torch.sum(C_i) * V\n",
    "        U_mean = U_ / M\n",
    "        V_mean = V_ / M\n",
    "        R_pred = torch.dot(U_mean + U, V.T + V_mean)\n",
    "        if P == 0:\n",
    "            P_pred = torch.zeros_like(self.P_matrix)\n",
    "        if Q == 0:\n",
    "            Q_pred = torch.zeros_like(self.Q_matrix)\n",
    "        if E == 0:\n",
    "            E = torch.zeros_like(self.E_matrix)\n",
    "        if D == 0:\n",
    "            D = torch.zeros_like(self.D_matrix)\n",
    "        loss = torch.sum((R - R_pred) ** 2) + torch.sum((P - P_pred) ** 2) + torch.sum((Q - Q_pred) ** 2)\n",
    "        loss += lambdas[0] * torch.sum(U ** 2) + lambdas[1] * torch.sum(V ** 2) + lambdas[2] * torch.sum(D ** 2) + lambdas[3] * torch.sum(E ** 2)\n",
    "        loss += b_u + b_v\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def predict(self):\n",
    "        self.factorize()\n",
    "        mmscaler = MinMaxScaler(feature_range=(1, 6))\n",
    "        R_pred = torch.matmul(self.U_matrix, self.V_matrix)\n",
    "        R_pred = R_pred.detach().numpy()\n",
    "        R_pred = mmscaler.fit_transform(R_pred).astype(int)\n",
    "        return R_pred\n",
    "\n",
    "    def evaluation(self, test):\n",
    "        import datetime\n",
    "\n",
    "        R_pred = self.predict()\n",
    "        print(R_pred)\n",
    "        mae_value = mae(R_pred, test)\n",
    "        rmse_value = rmse(R_pred, test)\n",
    "        top_10_f1_score_value = top_10_f1_score(R_pred, test)\n",
    "        top_10_precision_value = top_10_precision(R_pred, test)\n",
    "        top_1_precision_value = top_1_precision(R_pred, test)\n",
    "        precision_at_k_value = precision_at_k(R_pred, test)\n",
    "        result = pd.DataFrame()\n",
    "        result['RMSE'] = [rmse_value]\n",
    "        result['MAE'] = [mae_value]\n",
    "        result['top_10_f1_score'] = [top_10_f1_score_value]\n",
    "        result['top_10_precision'] = [top_10_precision_value]\n",
    "        result['top_1_precision'] = [top_1_precision_value]\n",
    "        result['precision_at_k'] = [precision_at_k_value]\n",
    "        print(result)\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        filename = f\"output_{timestamp}.csv\"\n",
    "        # result.to_csv(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_onehot.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.,  2.,  4., ..., nan, nan, nan],\n",
       "       [ 1.,  1.,  2., ..., nan, nan, nan],\n",
       "       [ 1., nan,  2., ..., nan, nan, nan],\n",
       "       ...,\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "zxz = k_fold_cross_validation(rating_matrix.replace(to_replace=0, value=np.nan).values)\n",
    "train_m=zxz[0][0]\n",
    "test_m =zxz[0][1]\n",
    "train_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       ...,\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "\n",
    "values = [1,  100,0.1]\n",
    "combinations = list(itertools.product(values, repeat=4))\n",
    "combinations_arr = np.array(combinations)\n",
    "selected_combinations = random.sample(combinations, 20)\n",
    "print(len(combinations_arr))\n",
    "model = NMF(n_components=20, init='random', random_state=0) #n_component = KNN values\n",
    "U_matrix = model.fit_transform(np.nan_to_num(train_m,0))\n",
    "V_matrix = model.components_\n",
    "# filter_D = model.fit_transform(df_onehot.values)\n",
    "# D_matrix = model.components_\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L_i=np.array(calculate_L(similarity_matrix))/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/200 [03:00<4:47:30, 87.12s/it] "
     ]
    }
   ],
   "source": [
    "param_grid = {'alpha': [0.01],\n",
    "              'lambdas_': selected_combinations,\n",
    "              'R_': [train_m],\n",
    "              'P_': [0],\n",
    "              'U_':[U_matrix],\n",
    "              'V_':[V_matrix],\n",
    "              'Q_': [0],\n",
    "              'E_': [0],\n",
    "              'D_': [0],\n",
    "              'C_u_': [C_umatrix],\n",
    "              'C_i_': [C_imatrix],\n",
    "              'L_U_': [0],\n",
    "              'L_V_': [0],\n",
    "              'b_u_': [user_bias.values],\n",
    "              'b_v_': [item_bias.values]}\n",
    "best_params = grid_search(MultiMF,param_grid, test_m)\n",
    "# alpha = best_params['alpha']\n",
    "# lambdas = best_params['lambdas_']\n",
    "# object = MultiMF(train_m,0,0,U_matrix,V_matrix,0,0,C_umatrix,C_imatrix,0,0,lambdas,user_bias.values,item_bias.values,alpha)\n",
    "# object.evaluation(test_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 288.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6 5 6 ... 4 4 4]\n",
      " [6 5 6 ... 6 5 5]\n",
      " [6 6 6 ... 6 6 6]\n",
      " ...\n",
      " [6 5 6 ... 6 5 5]\n",
      " [3 3 3 ... 3 3 3]\n",
      " [6 6 6 ... 6 6 6]]\n",
      "       RMSE       MAE  top_10_f1_score  top_10_precision  top_1_precision  \\\n",
      "0  2.124889  1.672176         0.002008          0.001846              0.0   \n",
      "\n",
      "   precision_at_k  \n",
      "0        0.001846  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "object = MultiMF(train_m,0,0,U_matrix,V_matrix.T,0,0,C_umatrix,C_imatrix,0,0,[0.1,0.01,0.1,0.001,0.01],user_bias.values,item_bias.values,0.01)\n",
    "# # object.factorize(5)\n",
    "object.evaluation(test_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 318.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(object.total_loss[1:])\n",
    "R_pred=object.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "482270248960.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "object.total_loss[1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010658622423328306"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_f1_score(R_pred,test_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_m.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmscaler = MinMaxScaler(feature_range=(1, 6))\n",
    "R_pred = torch.matmul(object.U_matrix.detach(), object.V_matrix.detach().T).numpy()\n",
    "R_pred = mmscaler.fit_transform(R_pred).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2147483648, -2147483648, -2147483648, ..., -2147483648,\n",
       "        -2147483648, -2147483648],\n",
       "       [-2147483648, -2147483648, -2147483648, ..., -2147483648,\n",
       "        -2147483648, -2147483648],\n",
       "       [-2147483648, -2147483648, -2147483648, ..., -2147483648,\n",
       "        -2147483648, -2147483648],\n",
       "       ...,\n",
       "       [-2147483648, -2147483648, -2147483648, ..., -2147483648,\n",
       "        -2147483648, -2147483648],\n",
       "       [-2147483648, -2147483648, -2147483648, ..., -2147483648,\n",
       "        -2147483648, -2147483648],\n",
       "       [-2147483648, -2147483648, -2147483648, ..., -2147483648,\n",
       "        -2147483648, -2147483648]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd26f88697b863e66c6ece3fc4ab8542eed3593bce93b36f41fad6131c5cf6bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
