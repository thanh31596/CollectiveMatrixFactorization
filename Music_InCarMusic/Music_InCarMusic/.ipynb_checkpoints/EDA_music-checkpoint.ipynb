{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from itertools import product, permutations\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "scaler_minmax=MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('music_full.csv').drop(columns=['Unnamed: 0'])\n",
    "origin_data= pd.read_excel('Data_InCarMusic.xlsx',sheet_name=0)\n",
    "df_1= pd.read_excel('Data_InCarMusic.xlsx',sheet_name=1)\n",
    "df_2= pd.read_excel('Data_InCarMusic.xlsx',sheet_name=2)\n",
    "df_3= pd.read_excel('Data_InCarMusic.xlsx',sheet_name=3)\n",
    "origin_data=origin_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import product\n",
    "\n",
    "# def grid_search(model, param_grid, n_iter=300):\n",
    "#     best_loss = float('inf')\n",
    "#     best_params = None\n",
    "#     for params in product(*param_grid.values()):\n",
    "#         params = dict(zip(param_grid.keys(), params))\n",
    "#         mf = model(**params)\n",
    "#         mf.factorize(iter=n_iter)\n",
    "#         loss = mf.total_loss[-1]\n",
    "#         if loss < best_loss:\n",
    "#             best_loss = loss\n",
    "#             best_params = params\n",
    "#     print('Best parameters:', best_params)\n",
    "#     print('Best loss:', best_loss)\n",
    "#     return best_params\n",
    "\n",
    "\n",
    "def grid_search(model, param_grid, test, filename='music_grid_search_results.xlsx'):\n",
    "    best_loss = float('inf')\n",
    "    best_params = None\n",
    "    results = []\n",
    "    for params in product(*param_grid.values()):\n",
    "        params = dict(zip(param_grid.keys(), params))\n",
    "        mf = model(**params)\n",
    "        mf.evaluation(test)\n",
    "        loss = mf.total_loss[-1]\n",
    "        results.append({**params, 'loss': loss})\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_params = params\n",
    "    print('Best parameters:', best_params)\n",
    "    print('Best loss:', best_loss)\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_excel(filename, index=False)\n",
    "    return best_params\n",
    "# import pandas as pd\n",
    "# from itertools import product\n",
    "# from multiprocessing import Process, Queue, cpu_count\n",
    "\n",
    "# def factorize_worker(model, params_list, n_iter, result_queue):\n",
    "#     for params in params_list:\n",
    "#         mf = model(**params)\n",
    "#         mf.factorize(iter=n_iter)\n",
    "#         loss = mf.total_loss[-1]\n",
    "#         result_queue.put({**params, 'loss': loss})\n",
    "\n",
    "# def grid_search(model, param_grid, n_iter=2, n_jobs=2, filename='grid_search_results.xlsx'):\n",
    "#     if n_jobs == -1:\n",
    "#         n_jobs = cpu_count()\n",
    "\n",
    "#     best_loss = float('inf')\n",
    "#     best_params = None\n",
    "#     results = []\n",
    "\n",
    "#     params_list = [dict(zip(param_grid.keys(), p)) for p in product(*param_grid.values())]\n",
    "#     result_queue = Queue()\n",
    "\n",
    "#     processes = []\n",
    "#     chunk_size = (len(params_list) + n_jobs - 1) // n_jobs\n",
    "\n",
    "#     for i in range(n_jobs):\n",
    "#         start = i * chunk_size\n",
    "#         end = min((i + 1) * chunk_size, len(params_list))\n",
    "#         p = Process(target=factorize_worker, args=(model, params_list[start:end], n_iter, result_queue))\n",
    "#         processes.append(p)\n",
    "\n",
    "#     for p in processes:\n",
    "#         p.start()\n",
    "\n",
    "#     for _ in range(len(params_list)):\n",
    "#         results.append(result_queue.get())\n",
    "\n",
    "#     for p in processes:\n",
    "#         p.join()\n",
    "\n",
    "#     df = pd.DataFrame(results)\n",
    "#     df.to_excel(filename, index=False)\n",
    "\n",
    "#     best_params = df.loc[df['loss'].idxmin()].to_dict()\n",
    "#     best_loss = best_params.pop('loss')\n",
    "\n",
    "#     print('Best parameters:', best_params)\n",
    "#     print('Best loss:', best_loss)\n",
    "\n",
    "#     return best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3127783487495718\n"
     ]
    }
   ],
   "source": [
    "def cal(df):\n",
    "\n",
    "    \n",
    "    # calculate total number of possible user-item interactions\n",
    "    num_users = df[df.columns[0]].nunique()\n",
    "    num_items = df[df.columns[1]].nunique()\n",
    "    num_possible_interactions = num_users * num_items\n",
    "    \n",
    "    # calculate total number of actual user-item interactions\n",
    "    num_actual_interactions = df.shape[0]\n",
    "    \n",
    "    # calculate sparsity of ratings\n",
    "    sparsity = 1 - (num_actual_interactions / num_possible_interactions)\n",
    "    \n",
    "    print(sparsity)\n",
    "cal(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def count_nan(df):\n",
    "    \"\"\"\n",
    "    Returns the percentage of NaN values in a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    total_cells = df.size\n",
    "    nan_cells = df.isna().sum().sum()\n",
    "    nan_percentage = (nan_cells / total_cells) * 100\n",
    "    print(nan_percentage)\n",
    "count_nan(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['UserID', 'ItemID', 'DrivingStyle', 'landscape', 'mood',\n",
       "       'naturalphenomena ', 'RoadType', 'sleepiness', 'trafficConditions',\n",
       "       'weather', 'Rating'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# neighbors=int(df['userid'].value_counts().mean())\n",
    "# imputer = KNNImputer(n_neighbors=neighbors)\n",
    "# imputed_df = pd.DataFrame(imputer.fit_transform(df),columns=['userid', 'itemid', 'rating', 'Time', 'Location', 'Companion'])\n",
    "\n",
    "# for i in imputed_df.columns: \n",
    "#     imputed_df[i] = imputed_df[i].astype('int')\n",
    "# imputed_df\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Contextual Coeficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1:\n",
      "  UserID: 0.013150549785903262\n",
      "  DrivingStyle: 0.0035029167040087385\n",
      "  landscape: -0.004444201573677493\n",
      "  mood: -0.004649624514019134\n",
      "  naturalphenomena : -0.008198778037263306\n",
      "  RoadType: 0.0038837340398165474\n",
      "  sleepiness: 0.007324540309988168\n",
      "  trafficConditions: -0.005481641075481302\n",
      "  weather: 0.00048702726909092376\n",
      "Class 2:\n",
      "  UserID: -0.0848996479424916\n",
      "  DrivingStyle: 0.006616413658154695\n",
      "  landscape: 0.0043011225197543725\n",
      "  mood: 0.0002358200265283165\n",
      "  naturalphenomena : 0.003566009833813204\n",
      "  RoadType: -0.0020729107745963756\n",
      "  sleepiness: -0.021552622932486186\n",
      "  trafficConditions: 0.01268439345310845\n",
      "  weather: -0.010410700236204993\n",
      "Class 3:\n",
      "  UserID: -0.05416695586656676\n",
      "  DrivingStyle: 0.014783259562516735\n",
      "  landscape: 5.140557554481432e-05\n",
      "  mood: -0.00630825525241535\n",
      "  naturalphenomena : 0.008475988592794153\n",
      "  RoadType: 0.007104641879077374\n",
      "  sleepiness: -0.0015535009873953378\n",
      "  trafficConditions: 0.002670182651420245\n",
      "  weather: 0.017964241426890622\n",
      "Class 4:\n",
      "  UserID: 0.038402598173039384\n",
      "  DrivingStyle: -0.00032449796770130096\n",
      "  landscape: -0.011032470555720696\n",
      "  mood: -0.013845334965632242\n",
      "  naturalphenomena : -0.004189032609253759\n",
      "  RoadType: -0.014675936620901796\n",
      "  sleepiness: -0.00025061880974449084\n",
      "  trafficConditions: -0.013197766777272728\n",
      "  weather: -0.023332103280717137\n",
      "Class 5:\n",
      "  UserID: 0.012664774266096829\n",
      "  DrivingStyle: -0.0016333305611191135\n",
      "  landscape: 0.005559336757220668\n",
      "  mood: 0.008579733538031271\n",
      "  naturalphenomena : -0.00011976231750569518\n",
      "  RoadType: -0.008234981219826907\n",
      "  sleepiness: 0.01841013430169249\n",
      "  trafficConditions: 0.0032532175623736755\n",
      "  weather: -0.003643482517796981\n",
      "Class 6:\n",
      "  UserID: 0.07484474475386768\n",
      "  DrivingStyle: -0.022949005448706476\n",
      "  landscape: 0.005569219464017826\n",
      "  mood: 0.015990430887450797\n",
      "  naturalphenomena : 0.00046460698463868136\n",
      "  RoadType: 0.01400287612902223\n",
      "  sleepiness: -0.002383796015611456\n",
      "  trafficConditions: 7.718983323078309e-05\n",
      "  weather: 0.018933879979966657\n",
      "Class 1:\n",
      "  ItemID: 0.003515804855966148\n",
      "  DrivingStyle: 0.0037323447871563456\n",
      "  landscape: -0.004709645737427505\n",
      "  mood: -0.004919305826490027\n",
      "  naturalphenomena : -0.008575194728772134\n",
      "  RoadType: 0.004079068952459262\n",
      "  sleepiness: 0.0074344290921041735\n",
      "  trafficConditions: -0.00536470750106412\n",
      "  weather: 0.00014297730785900657\n",
      "Class 2:\n",
      "  ItemID: 0.18302069176481428\n",
      "  DrivingStyle: 0.013227972697592122\n",
      "  landscape: 0.010520179902451191\n",
      "  mood: 0.0037880704519878215\n",
      "  naturalphenomena : 0.010762070036927257\n",
      "  RoadType: -0.007463772346593629\n",
      "  sleepiness: -0.012970567661162353\n",
      "  trafficConditions: 0.0126329210992513\n",
      "  weather: -0.008242453388465219\n",
      "Class 3:\n",
      "  ItemID: 0.014874476046828478\n",
      "  DrivingStyle: 0.014978645772356875\n",
      "  landscape: 0.0017757236572933832\n",
      "  mood: -0.00495475422900501\n",
      "  naturalphenomena : 0.010701780235333901\n",
      "  RoadType: 0.005722261999215969\n",
      "  sleepiness: -0.0006752883281377328\n",
      "  trafficConditions: 0.0022953866377416936\n",
      "  weather: 0.01936089493891075\n",
      "Class 4:\n",
      "  ItemID: -0.020785220588297116\n",
      "  DrivingStyle: -0.0008638359471492264\n",
      "  landscape: -0.012483469859125015\n",
      "  mood: -0.014898725575871144\n",
      "  naturalphenomena : -0.006004325767362683\n",
      "  RoadType: -0.013493792167056205\n",
      "  sleepiness: -0.0013308555634592672\n",
      "  trafficConditions: -0.012970355979682834\n",
      "  weather: -0.024320440526687137\n",
      "Class 5:\n",
      "  ItemID: -0.051164827271979935\n",
      "  DrivingStyle: -0.003562769463403566\n",
      "  landscape: 0.004107140171289722\n",
      "  mood: 0.007836506699148482\n",
      "  naturalphenomena : -0.001747530814053373\n",
      "  RoadType: -0.006947587976381809\n",
      "  sleepiness: 0.016051958263925192\n",
      "  trafficConditions: 0.003182759429521865\n",
      "  weather: -0.003964106914012948\n",
      "Class 6:\n",
      "  ItemID: -0.1294601878914419\n",
      "  DrivingStyle: -0.02751720366786703\n",
      "  landscape: 0.000787384414539751\n",
      "  mood: 0.013143587944321232\n",
      "  naturalphenomena : -0.005138467302470053\n",
      "  RoadType: 0.018109500598911043\n",
      "  sleepiness: -0.00850926372396604\n",
      "  trafficConditions: 0.000227579312837558\n",
      "  weather: 0.01701900383705546\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming your dataframe is called df\n",
    "X = df[['UserID', 'DrivingStyle', 'landscape', 'mood',\n",
    "       'naturalphenomena ', 'RoadType', 'sleepiness', 'trafficConditions',\n",
    "       'weather', ]]\n",
    "y = df['Rating']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit the LinearSVC model\n",
    "svm = LinearSVC(random_state=42)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = svm.coef_\n",
    "\n",
    "# Print the feature importances\n",
    "features = ['UserID', 'DrivingStyle', 'landscape', 'mood',\n",
    "       'naturalphenomena ', 'RoadType', 'sleepiness', 'trafficConditions',\n",
    "       'weather', ]\n",
    "for i in range(importances.shape[0]):\n",
    "    print(f'Class {i+1}:')\n",
    "    for j in range(importances.shape[1]):\n",
    "        print(f'  {features[j]}: {importances[i, j]}')\n",
    "\n",
    "# Calculate the mean importance for each feature across all classes\n",
    "mean_importances = np.mean(importances, axis=0)\n",
    "\n",
    "# Create a dictionary to map feature names to mean importances\n",
    "feature_importance_dict = {feature: importance for feature, importance in zip(features, mean_importances)}\n",
    "\n",
    "# Replace the values in 'Time', 'Location', and 'Companion' columns with their respective feature importances\n",
    "df_replaced =df.copy()\n",
    "for feature in features:\n",
    "    df_replaced[feature] = df_replaced[feature] * feature_importance_dict[feature]\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# X = imputed_df[['Time', 'Location', 'Companion']]\n",
    "# y = imputed_df['rating']\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Standardize the features\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Fit the AdaBoost model with a DecisionTreeClassifier as the base estimator\n",
    "# base_estimator = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "# ada = AdaBoostClassifier(base_estimator=base_estimator, random_state=42)\n",
    "# ada.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Get feature importances\n",
    "# importances = ada.feature_importances_\n",
    "\n",
    "# # Print the feature importances\n",
    "# features = ['Time', 'Location', 'Companion']\n",
    "# for i, importance in enumerate(importances):\n",
    "#     print(f'{features[i]}: {importance}')\n",
    "\n",
    "X = df[['ItemID', 'DrivingStyle', 'landscape', 'mood',\n",
    "       'naturalphenomena ', 'RoadType', 'sleepiness', 'trafficConditions',\n",
    "       'weather', ]]\n",
    "y = df['Rating']\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit the LinearSVC model\n",
    "svm = LinearSVC(random_state=42)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = svm.coef_\n",
    "\n",
    "# Print the feature importances\n",
    "features = ['ItemID', 'DrivingStyle', 'landscape', 'mood',\n",
    "       'naturalphenomena ', 'RoadType', 'sleepiness', 'trafficConditions',\n",
    "       'weather', ]\n",
    "for i in range(importances.shape[0]):\n",
    "    print(f'Class {i+1}:')\n",
    "    for j in range(importances.shape[1]):\n",
    "        print(f'  {features[j]}: {importances[i, j]}')\n",
    "\n",
    "# Calculate the mean importance for each feature across all classes\n",
    "mean_importances = np.mean(importances, axis=0)\n",
    "\n",
    "# Create a dictionary to map feature names to mean importances\n",
    "feature_importance_dict = {feature: importance for feature, importance in zip(features, mean_importances)}\n",
    "\n",
    "# Replace the values in 'Time', 'Location', and 'Companion' columns with their respective feature importances\n",
    "df_replaced_item =df.copy()\n",
    "for feature in features:\n",
    "    df_replaced_item[feature] = df_replaced_item[feature] * feature_importance_dict[feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ItemID</th>\n",
       "      <th>DrivingStyle</th>\n",
       "      <th>landscape</th>\n",
       "      <th>mood</th>\n",
       "      <th>naturalphenomena</th>\n",
       "      <th>RoadType</th>\n",
       "      <th>sleepiness</th>\n",
       "      <th>trafficConditions</th>\n",
       "      <th>weather</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>715</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>267</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>294</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "      <td>259</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>674</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007</th>\n",
       "      <td>1042</td>\n",
       "      <td>716</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4008</th>\n",
       "      <td>1042</td>\n",
       "      <td>733</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4009</th>\n",
       "      <td>1042</td>\n",
       "      <td>682</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4010</th>\n",
       "      <td>1042</td>\n",
       "      <td>691</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4011</th>\n",
       "      <td>1042</td>\n",
       "      <td>723</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4012 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      UserID  ItemID  DrivingStyle  landscape  mood  naturalphenomena   \\\n",
       "0       1001     715             1          2     2                  2   \n",
       "1       1001     267             1          2     4                  4   \n",
       "2       1001     294             1          2     4                  2   \n",
       "3       1001     259             1          4     4                  2   \n",
       "4       1001     674             1          4     2                  2   \n",
       "...      ...     ...           ...        ...   ...                ...   \n",
       "4007    1042     716             2          4     1                  2   \n",
       "4008    1042     733             1          2     4                  4   \n",
       "4009    1042     682             2          2     4                  4   \n",
       "4010    1042     691             1          2     2                  2   \n",
       "4011    1042     723             1          2     4                  4   \n",
       "\n",
       "      RoadType  sleepiness  trafficConditions  weather  Rating  \n",
       "0            2           1                  3        4       2  \n",
       "1            2           2                  3        2       4  \n",
       "2            3           1                  3        2       2  \n",
       "3            3           1                  3        4       4  \n",
       "4            3           1                  3        1       2  \n",
       "...        ...         ...                ...      ...     ...  \n",
       "4007         3           1                  1        2       1  \n",
       "4008         3           1                  3        1       1  \n",
       "4009         3           1                  1        2       1  \n",
       "4010         3           1                  3        4       1  \n",
       "4011         3           1                  3        4       1  \n",
       "\n",
       "[4012 rows x 11 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fix_user = df_replaced[[ 'DrivingStyle', 'landscape', 'mood',\n",
    "       'naturalphenomena ', 'RoadType', 'sleepiness', 'trafficConditions',\n",
    "       'weather', ]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix_user = df_replaced[['DrivingStyle', 'landscape', 'mood',\n",
    "       'naturalphenomena ', 'RoadType', 'sleepiness', 'trafficConditions',\n",
    "       'weather', ]]\n",
    "for i in ['DrivingStyle', 'landscape', 'mood','naturalphenomena ', 'RoadType', 'sleepiness', 'trafficConditions','weather', ]:\n",
    "    for j in range(len(df_fix_user[i])):\n",
    "        if origin_data[i][j] ==0: \n",
    "            df_replaced[i][j]=0\n",
    "df_fix_user = df_replaced_item[['DrivingStyle', 'landscape', 'mood','naturalphenomena ', 'RoadType', 'sleepiness', 'trafficConditions','weather', ]]\n",
    "for i in ['DrivingStyle', 'landscape', 'mood','naturalphenomena ', 'RoadType', 'sleepiness', 'trafficConditions','weather', ]:\n",
    "    for j in range(len(df_fix_user[i])):\n",
    "        if origin_data[i][j] ==0: \n",
    "            df_replaced_item[i][j]=0      \n",
    "for i in df_fix_user.columns: \n",
    "    prefix = \"_user\"\n",
    "    # create a new column name by adding the prefix to the original column name\n",
    "    new_col = i+prefix\n",
    "    # create the new column by copying the original column data to the new column\n",
    "    df[new_col] = df_replaced[i]\n",
    "for i in df_fix_user.columns: \n",
    "    prefix = \"_item\"\n",
    "    # create a new column name by adding the prefix to the original column name\n",
    "    new_col = i+prefix\n",
    "    # create the new column by copying the original column data to the new column\n",
    "    df[new_col] = df_replaced_item[i]\n",
    "try:\n",
    "    df=df.drop(columns=['Unnamed: 0'])\n",
    "except KeyError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ItemID</th>\n",
       "      <th>DrivingStyle</th>\n",
       "      <th>landscape</th>\n",
       "      <th>mood</th>\n",
       "      <th>naturalphenomena</th>\n",
       "      <th>RoadType</th>\n",
       "      <th>sleepiness</th>\n",
       "      <th>trafficConditions</th>\n",
       "      <th>weather</th>\n",
       "      <th>...</th>\n",
       "      <th>trafficConditions_user</th>\n",
       "      <th>weather_user</th>\n",
       "      <th>DrivingStyle_item</th>\n",
       "      <th>landscape_item</th>\n",
       "      <th>mood_item</th>\n",
       "      <th>naturalphenomena _item</th>\n",
       "      <th>RoadType_item</th>\n",
       "      <th>sleepiness_item</th>\n",
       "      <th>trafficConditions_item</th>\n",
       "      <th>weather_item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>715</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.582392e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.749830e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>267</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.791196e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.374915e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>294</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.791196e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.374915e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "      <td>259</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.582392e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.749830e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>674</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.895598e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.874576e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007</th>\n",
       "      <td>1042</td>\n",
       "      <td>716</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4008</th>\n",
       "      <td>1042</td>\n",
       "      <td>733</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4009</th>\n",
       "      <td>1042</td>\n",
       "      <td>682</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4010</th>\n",
       "      <td>1042</td>\n",
       "      <td>691</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4011</th>\n",
       "      <td>1042</td>\n",
       "      <td>723</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4012 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      UserID  ItemID  DrivingStyle  landscape  mood  naturalphenomena   \\\n",
       "0       1001     715             1          2     2                  2   \n",
       "1       1001     267             1          2     4                  4   \n",
       "2       1001     294             1          2     4                  2   \n",
       "3       1001     259             1          4     4                  2   \n",
       "4       1001     674             1          4     2                  2   \n",
       "...      ...     ...           ...        ...   ...                ...   \n",
       "4007    1042     716             2          4     1                  2   \n",
       "4008    1042     733             1          2     4                  4   \n",
       "4009    1042     682             2          2     4                  4   \n",
       "4010    1042     691             1          2     2                  2   \n",
       "4011    1042     723             1          2     4                  4   \n",
       "\n",
       "      RoadType  sleepiness  trafficConditions  weather  ...  \\\n",
       "0            2           1                  3        4  ...   \n",
       "1            2           2                  3        2  ...   \n",
       "2            3           1                  3        2  ...   \n",
       "3            3           1                  3        4  ...   \n",
       "4            3           1                  3        1  ...   \n",
       "...        ...         ...                ...      ...  ...   \n",
       "4007         3           1                  1        2  ...   \n",
       "4008         3           1                  3        1  ...   \n",
       "4009         3           1                  1        2  ...   \n",
       "4010         3           1                  3        4  ...   \n",
       "4011         3           1                  3        4  ...   \n",
       "\n",
       "      trafficConditions_user  weather_user  DrivingStyle_item  landscape_item  \\\n",
       "0                        0.0 -7.582392e-07                0.0             0.0   \n",
       "1                        0.0 -3.791196e-07                0.0             0.0   \n",
       "2                        0.0 -3.791196e-07                0.0             0.0   \n",
       "3                        0.0 -7.582392e-07                0.0             0.0   \n",
       "4                        0.0 -1.895598e-07                0.0             0.0   \n",
       "...                      ...           ...                ...             ...   \n",
       "4007                     0.0  0.000000e+00                0.0             0.0   \n",
       "4008                     0.0  0.000000e+00                0.0             0.0   \n",
       "4009                     0.0  0.000000e+00                0.0             0.0   \n",
       "4010                     0.0  0.000000e+00                0.0             0.0   \n",
       "4011                     0.0  0.000000e+00                0.0             0.0   \n",
       "\n",
       "      mood_item  naturalphenomena _item  RoadType_item  sleepiness_item  \\\n",
       "0           0.0                     0.0            0.0              0.0   \n",
       "1           0.0                     0.0            0.0              0.0   \n",
       "2           0.0                     0.0            0.0              0.0   \n",
       "3           0.0                     0.0            0.0              0.0   \n",
       "4           0.0                     0.0            0.0              0.0   \n",
       "...         ...                     ...            ...              ...   \n",
       "4007        0.0                     0.0            0.0              0.0   \n",
       "4008        0.0                     0.0            0.0              0.0   \n",
       "4009        0.0                     0.0            0.0              0.0   \n",
       "4010        0.0                     0.0            0.0              0.0   \n",
       "4011        0.0                     0.0            0.0              0.0   \n",
       "\n",
       "      trafficConditions_item  weather_item  \n",
       "0                        0.0 -2.749830e-06  \n",
       "1                        0.0 -1.374915e-06  \n",
       "2                        0.0 -1.374915e-06  \n",
       "3                        0.0 -2.749830e-06  \n",
       "4                        0.0 -6.874576e-07  \n",
       "...                      ...           ...  \n",
       "4007                     0.0  0.000000e+00  \n",
       "4008                     0.0  0.000000e+00  \n",
       "4009                     0.0  0.000000e+00  \n",
       "4010                     0.0  0.000000e+00  \n",
       "4011                     0.0  0.000000e+00  \n",
       "\n",
       "[4012 rows x 27 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.3776171485543371\n",
      "1.6223828514456629\n",
      "0.05798096467924409\n",
      "SUM:  2.4352005165282518\n",
      "-1.2526171485543371\n",
      "1.705716184778996\n",
      "0.015886554976344825\n",
      "SUM:  2.2082311417119307\n"
     ]
    }
   ],
   "source": [
    "df_replaced.head(30)\n",
    "global_mean = df_replaced[\"Rating\"].mean()\n",
    "user_bias = df_replaced.groupby(\"UserID\")[\"Rating\"].mean() - global_mean\n",
    "item_bias = df_replaced.groupby(\"ItemID\")[\"Rating\"].mean() - global_mean\n",
    "print(np.min(user_bias))\n",
    "print(np.max(user_bias))\n",
    "print(np.mean(user_bias))\n",
    "print(\"SUM: \",np.sum(user_bias))\n",
    "print(np.min(item_bias))\n",
    "print(np.max(item_bias))\n",
    "print(np.mean(item_bias))\n",
    "print(\"SUM: \",np.sum(item_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', ' album', ' artist', 'title', ' mp3url', ' description',\n",
       "       ' imageurl', 'category_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rating_matrix_original= df[['UserID','ItemID','Rating']].pivot_table(values='Rating',index='UserID',columns='ItemID', fill_value=0).astype('int')\n",
    "rating_matrix_original.values\n",
    "df_2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate real Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['UserID', 'ItemID', 'DrivingStyle', 'landscape', 'mood',\n",
       "       'naturalphenomena ', 'RoadType', 'sleepiness', 'trafficConditions',\n",
       "       'weather', 'Rating', 'DrivingStyle_user', 'landscape_user', 'mood_user',\n",
       "       'naturalphenomena _user', 'RoadType_user', 'sleepiness_user',\n",
       "       'trafficConditions_user', 'weather_user', 'DrivingStyle_item',\n",
       "       'landscape_item', 'mood_item', 'naturalphenomena _item',\n",
       "       'RoadType_item', 'sleepiness_item', 'trafficConditions_item',\n",
       "       'weather_item'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def to_int(x):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    return int(x)\n",
    "rating_matrix=rating_matrix_original\n",
    "df_onehot = pd.get_dummies(df_2[['category_id', ' artist']])\n",
    "similarity_matrix = cosine_similarity(df_onehot)\n",
    "def calculate_L(similarity_matrix):\n",
    "    L = []\n",
    "\n",
    "    for i in range(len(similarity_matrix)):\n",
    "        a = 0\n",
    "        for j in range(len(similarity_matrix)):\n",
    "            a = similarity_matrix[i][j] * np.sum((similarity_matrix[i] - similarity_matrix[j]))\n",
    "        L.append(a)\n",
    "    return L\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3080357272430603\n",
      "0.28395302773143866\n",
      "0.0\n",
      "SUM:  -135.9409770042133\n",
      "-0.19547111856049565\n",
      "0.37117162955367655\n",
      "0.0\n",
      "SUM:  296.225649126917\n"
     ]
    }
   ],
   "source": [
    "neighbors=52\n",
    "\n",
    "# Compute adjacency matrices based on similarity\n",
    "# user_adj_matrix = adjacency_matrix_similarity(U_matrix)\n",
    "# item_adj_matrix = adjacency_matrix_similarity(V_matrix)\n",
    "# # Compute Laplacian matrices\n",
    "# L_U = laplacian_matrix(user_adj_matrix)\n",
    "# L_V = laplacian_matrix(item_adj_matrix)\n",
    "# Get context matrix\n",
    "scaler = StandardScaler()\n",
    "# minmax = MinMaxScaler(feature_range=(0,2))\n",
    "Cu=df[['DrivingStyle_user', 'landscape_user', 'mood_user','naturalphenomena _user', 'RoadType_user', 'sleepiness_user','trafficConditions_user', 'weather_user']].multiply(100000)\n",
    "Ci=df[['DrivingStyle_item',\t'landscape_item', 'mood_item', 'naturalphenomena _item','RoadType_item', 'sleepiness_item', 'trafficConditions_item','weather_item']].multiply(100000)\n",
    "C_umatrix =  np.array(Cu)\n",
    "C_imatrix =  np.array(Ci)\n",
    "print(np.min(C_imatrix))\n",
    "print(np.max(C_imatrix))\n",
    "print(np.median(C_imatrix))\n",
    "print(\"SUM: \",np.sum(C_imatrix))\n",
    "print(np.min(C_umatrix))\n",
    "print(np.max(C_umatrix))\n",
    "print(np.median(C_umatrix))\n",
    "print(\"SUM: \",np.sum(C_umatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predicted_ratings, real_ratings):\n",
    "    # Create a mask of the same shape as real_ratings with True where there's a rating and False where there's NaN\n",
    "    mask = ~np.isnan(real_ratings)\n",
    "\n",
    "    # Calculate the squared error between the predicted and real ratings only for the rated items\n",
    "    squared_error = (predicted_ratings[mask] - real_ratings[mask])**2\n",
    "\n",
    "    # Calculate the mean squared error\n",
    "    mean_squared_error = np.mean(squared_error)\n",
    "\n",
    "    # Calculate the root mean squared error\n",
    "    root_mean_squared_error = np.sqrt(mean_squared_error)\n",
    "\n",
    "    return root_mean_squared_error\n",
    "\n",
    "def mae(predicted_ratings, real_ratings):\n",
    "    # Create a mask of the same shape as real_ratings with True where there's a rating and False where there's NaN\n",
    "    mask = ~np.isnan(real_ratings)\n",
    "\n",
    "    # Calculate the absolute error between the predicted and real ratings only for the rated items\n",
    "    absolute_error = np.abs(predicted_ratings[mask] - real_ratings[mask])\n",
    "\n",
    "    # Calculate the mean absolute error\n",
    "    mean_absolute_error = np.mean(absolute_error)\n",
    "\n",
    "    return mean_absolute_error\n",
    "def top_10_f1_score(predicted_ratings, real_ratings):\n",
    "    f1_sum = 0\n",
    "    users_count = real_ratings.shape[0]\n",
    "\n",
    "    for user_idx in range(users_count):\n",
    "        user_real_ratings = real_ratings[user_idx]\n",
    "        user_predicted_ratings = predicted_ratings[user_idx]\n",
    "\n",
    "        # Get the indices of the top-10 predicted ratings\n",
    "        top_10_predicted_indices = np.argsort(user_predicted_ratings)[-10:]\n",
    "\n",
    "        # Get the indices of the user's real ratings\n",
    "        real_rated_indices = np.where(~np.isnan(user_real_ratings))[0]\n",
    "\n",
    "        # Calculate the number of relevant items in the top-10 predicted items\n",
    "        relevant_items_count = np.sum(np.isin(top_10_predicted_indices, real_rated_indices))\n",
    "\n",
    "        # Calculate the precision for the current user\n",
    "        user_precision = relevant_items_count / 10\n",
    "\n",
    "        # Calculate the recall for the current user\n",
    "        user_recall = relevant_items_count / len(real_rated_indices)\n",
    "\n",
    "        # Calculate the F1-score for the current user\n",
    "        if user_precision + user_recall > 0:\n",
    "            user_f1_score = 2 * user_precision * user_recall / (user_precision + user_recall)\n",
    "        else:\n",
    "            user_f1_score = 0\n",
    "\n",
    "        # Update the F1-score sum\n",
    "        f1_sum += user_f1_score\n",
    "\n",
    "    # Calculate the average F1-score across all users\n",
    "    average_f1_score = f1_sum / users_count\n",
    "\n",
    "    return average_f1_score\n",
    "def top_10_precision(predicted_ratings, real_ratings):\n",
    "    precision_sum = 0\n",
    "    users_count = real_ratings.shape[0]\n",
    "\n",
    "    for user_idx in range(users_count):\n",
    "        user_real_ratings = real_ratings[user_idx]\n",
    "        user_predicted_ratings = predicted_ratings[user_idx]\n",
    "\n",
    "        # Get the indices of the top-10 predicted ratings\n",
    "        top_10_predicted_indices = np.argsort(user_predicted_ratings)[-10:]\n",
    "\n",
    "        # Get the indices of the user's real ratings\n",
    "        real_rated_indices = np.where(~np.isnan(user_real_ratings))[0]\n",
    "\n",
    "        # Calculate the number of relevant items in the top-10 predicted items\n",
    "        relevant_items_count = np.sum(np.isin(top_10_predicted_indices, real_rated_indices))\n",
    "\n",
    "        # Calculate the precision for the current user\n",
    "        user_precision = relevant_items_count / 10\n",
    "\n",
    "        # Update the precision sum\n",
    "        precision_sum += user_precision\n",
    "\n",
    "    # Calculate the average precision across all users\n",
    "    average_precision = precision_sum / users_count\n",
    "\n",
    "    return average_precision\n",
    "\n",
    "def top_1_precision(predicted_ratings, real_ratings):\n",
    "    precision_sum = 0\n",
    "    users_count = real_ratings.shape[0]\n",
    "\n",
    "    for user_idx in range(users_count):\n",
    "        user_real_ratings = real_ratings[user_idx]\n",
    "        user_predicted_ratings = predicted_ratings[user_idx]\n",
    "\n",
    "        # Get the indices of the top-10 predicted ratings\n",
    "        top_10_predicted_indices = np.argsort(user_predicted_ratings)[-1:]\n",
    "\n",
    "        # Get the indices of the user's real ratings\n",
    "        real_rated_indices = np.where(~np.isnan(user_real_ratings))[0]\n",
    "\n",
    "        # Calculate the number of relevant items in the top-10 predicted items\n",
    "        relevant_items_count = np.sum(np.isin(top_10_predicted_indices, real_rated_indices))\n",
    "\n",
    "        # Calculate the precision for the current user\n",
    "        user_precision = relevant_items_count \n",
    "\n",
    "        # Update the precision sum\n",
    "        precision_sum += user_precision\n",
    "\n",
    "    # Calculate the average precision across all users\n",
    "    average_precision = precision_sum / users_count\n",
    "\n",
    "    return average_precision\n",
    "def precision_at_k(predicted_ratings, real_ratings, k=10):\n",
    "    precision_sum = 0\n",
    "    users_count = real_ratings.shape[0]\n",
    "\n",
    "    for user_idx in range(users_count):\n",
    "        user_real_ratings = real_ratings[user_idx]\n",
    "        user_predicted_ratings = predicted_ratings[user_idx]\n",
    "\n",
    "        # Get the indices of the top-k predicted ratings\n",
    "        top_k_predicted_indices = np.argsort(user_predicted_ratings)[-k:]\n",
    "\n",
    "        # Get the indices of the user's real ratings\n",
    "        real_rated_indices = np.where(~np.isnan(user_real_ratings))[0]\n",
    "\n",
    "        # Calculate the number of relevant items in the top-k predicted items\n",
    "        relevant_items_count = np.sum(np.isin(top_k_predicted_indices, real_rated_indices))\n",
    "\n",
    "        # Calculate the precision for the current user\n",
    "        user_precision = relevant_items_count / k\n",
    "\n",
    "        # Update the precision sum\n",
    "        precision_sum += user_precision\n",
    "\n",
    "    # Calculate the average precision across all users\n",
    "    average_precision = precision_sum / users_count\n",
    "\n",
    "    return average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def k_fold_cross_validation(ratings, k=5, random_state=None):\n",
    "    # Create a KFold object with the specified number of folds\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Initialize the list to store the train and test matrices for each fold\n",
    "    train_test_matrices = []\n",
    "\n",
    "    # Iterate over the splits\n",
    "    for train_indices, test_indices in kf.split(ratings):\n",
    "        # Copy the original ratings matrix for both train and test matrices\n",
    "        train_matrix = ratings.copy()\n",
    "        test_matrix = np.empty_like(ratings)\n",
    "\n",
    "        test_matrix[:] = np.nan\n",
    "\n",
    "        # Replace the test ratings with NaN in the train matrix and vice versa\n",
    "        for row, col in zip(*np.nonzero(ratings)):\n",
    "            if row in test_indices:\n",
    "                train_matrix[row, col] = np.nan\n",
    "                test_matrix[row, col] = ratings[row, col]\n",
    "\n",
    "        # Add the train and test matrices to the list\n",
    "        train_test_matrices.append((train_matrix, test_matrix))\n",
    "\n",
    "    return train_test_matrices\n",
    "\n",
    "class MultiMF: \n",
    "    def __init__(self,R_, P_, Q_,U_,V_,  E_, D_, C_u_,C_i_,  L_U_, L_V_, lambdas_, b_u_, b_v_,alpha,):\n",
    "\n",
    "        self.R_matrix = np.array(R_)\n",
    "        self.P_matrix = np.array(P_) \n",
    "        self.Q_matrix = np.array(Q_)\n",
    "        self.lambdas = lambdas_ \n",
    "        self.U_matrix = np.array(U_)\n",
    "        self.V_matrix = np.array(V_)\n",
    "        self.E_matrix = np.array(E_)\n",
    "        self.D_matrix = np.array(D_) \n",
    "        self.C_umatrix=np.array(C_u_) \n",
    "        self.C_imatrix=np.array(C_i_) \n",
    "        self.L_U=L_U_\n",
    "        self.L_V=L_V_\n",
    "        self.item_bias=np.array(b_v_)\n",
    "        self.user_bias=np.array(b_u_)\n",
    "        self.newU=[]\n",
    "        self.newV=[]\n",
    "        self.newE=[]\n",
    "        self.newD=[]\n",
    "        self.alpha=alpha\n",
    "        self.total_loss=[0]\n",
    "    def factorize(self,iter=500):\n",
    "        self.run_func()\n",
    "        for k in tqdm(range(iter)):\n",
    "            if abs(self.total_loss[-1] - self.total_loss[-2]) < 0.0004:\n",
    "                print(\"Success\")\n",
    "                break\n",
    "            else: \n",
    "                # self.U_matrix=np.array(self.newU)\n",
    "                # self.V_matrix=np.array(self.newV).T\n",
    "                # self.D_matrix=np.array(self.newD)\n",
    "                # self.E_matrix=np.array(self.newE)\n",
    "                # self.newU=[]\n",
    "                # self.newV=[]\n",
    "                # self.newE=[]\n",
    "                # self.newD=[]\n",
    "                self.run_func()\n",
    "        # return self.U_matrix, self.V_matrix\n",
    "    def run_func(self): \n",
    "        loss_col = 0 \n",
    "        lambdas=self.lambdas\n",
    "        for i in range(len(self.R_matrix)):  #70\n",
    "            loss_row = 0\n",
    "            U = self.U_matrix[i]\n",
    "            C_u = self.C_umatrix[i,:]\n",
    "            C_i = self.C_imatrix[i,:]\n",
    "            b_u=self.user_bias[i]\n",
    "            for j in range(len(self.R_matrix[i])): #97\n",
    "\n",
    "                if not np.isnan(self.R_matrix[i,j]):\n",
    "\n",
    "                    R = self.R_matrix[i,j]\n",
    "                    V = self.V_matrix.T[j]\n",
    "                    b_v=self.item_bias[j]\n",
    "                    #First situation\n",
    "                    P=Q=L_U=L_V=E=D=0\n",
    "                    Li = self.L_V[j]\n",
    "                    U, V, D, E = self.update_U_V_E_D(R=R, P=0, E=0, D=0, Q=0, U=U, V=V, C_u=C_u, C_i=C_i,Lu=0,Li=Li, lambdas=lambdas)\n",
    "                    self.U_matrix[i] = U\n",
    "                    self.V_matrix.T[j] = V\n",
    "                    loss,U_o,V_o,D_o,E_o = self.objective_function(R, P, Q, U, V, E, D, C_u,C_i,  L_U, L_V, lambdas, b_u, b_v)\n",
    "\n",
    "\n",
    "                    loss_row+=loss\n",
    "                    # U_ = self.update_U(R, P, Q, U, V, E, D, C_u, C_i, lambdas, alpha)\n",
    "            loss_col+=loss_row\n",
    "        self.total_loss.append(loss_col)\n",
    "    def objective_function(self,R, P, Q, U, V, E, D, C_u,C_i,  L_U, L_V, lambdas, b_u, b_v):\n",
    "        # U=U+0.0000000000005\n",
    "        # V=V+0.0000000000005\n",
    "        M = U.shape[0] #number of user\n",
    "        V_=0\n",
    "        U_=0\n",
    "        \n",
    "        U_ += np.sum(C_u)*U\n",
    "        V_ += np.sum(C_i)*V\n",
    "        U_mean=U_/M\n",
    "        V_mean=V_/M\n",
    "        R_pred = np.dot(U_mean + U, V.T + V_mean)\n",
    "        if P !=0:\n",
    "            P_pred = np.dot(U, E.T)\n",
    "            Q_pred = np.dot(V, D.T)\n",
    "        else:\n",
    "            P_pred = 0\n",
    "            Q_pred = 0\n",
    "        loss = np.sum((R - R_pred)**2) + np.sum((P - P_pred)**2) + np.sum((Q - Q_pred)**2)\n",
    "        loss += lambdas[0] * np.sum(U**2) + lambdas[1] * np.sum(V**2) + lambdas[2] * np.sum(D**2) + lambdas[3] * np.sum(E**2)\n",
    "        loss += b_u + b_v\n",
    "        \n",
    "        return loss,U,V,D,E\n",
    "\n",
    "    def update_U_V_E_D(self,R, P, Q, U, V, E, D, C_u,C_i,Lu,Li,lambdas):\n",
    "        alpha=self.alpha\n",
    "        # U=U+0.0000000000005\n",
    "        # V=V+0.0000000000005\n",
    "        M = U.shape[0] #number of user\n",
    "        V_=0\n",
    "        U_=0\n",
    "        U_ += np.sum(C_u)*U\n",
    "        V_ += np.sum(C_i)*V\n",
    "        U_mean=U_/M\n",
    "\n",
    "        V_mean=V_/M\n",
    "        if P != 0:\n",
    "            P_pred = np.dot(U, E.T)\n",
    "            Q_pred = np.dot(V, D.T)\n",
    "        else:\n",
    "            P_pred = 0\n",
    "            Q_pred = 0\n",
    "        R_pred = np.dot((U_mean + U), (V.T + V_mean))\n",
    "\n",
    "\n",
    "        dU = -2 * (R- R_pred) * (V.T + V_mean)*(1+U_mean) - 2 * (P - P_pred) * E + 2 * lambdas[0] * U   + 2*lambdas[3]*Lu #change to lambda 5 soon\n",
    "        dV = -2 * (R - R_pred) * (U_mean + U)*(1+V_mean) - 2 * (Q - Q_pred) * D + 2 * lambdas[1] * V  + 2*lambdas[4]*Li\n",
    "        dE = -2 * (P - P_pred) * U + 2 * lambdas[3] * E\n",
    "        dD = -2 * (Q - Q_pred) * V + 2 * lambdas[4] * D\n",
    "        \n",
    "        U -= alpha * dU\n",
    "        V -= alpha * dV\n",
    "        E -= alpha * dE\n",
    "        D -= alpha * dD\n",
    "\n",
    "\n",
    "        return U,V,E,D\n",
    "\n",
    "    def predict(self):\n",
    "        self.factorize()\n",
    "        mmscaler=MinMaxScaler(feature_range=(1, 6))\n",
    "        R_pred = np.dot(self.U_matrix,self.V_matrix)\n",
    "        R_pred=mmscaler.fit_transform(R_pred).astype(int)\n",
    "        return R_pred\n",
    "    def evaluation(self,test):\n",
    "        import datetime\n",
    "\n",
    "        R_pred = self.predict()\n",
    "        print(R_pred)\n",
    "        mae_value = mae(R_pred,test)\n",
    "        rmse_value = rmse(R_pred,test)\n",
    "        top_10_f1_score_value = top_10_f1_score(R_pred,test)\n",
    "        top_10_precision_value = top_10_precision(R_pred,test)\n",
    "        top_1_precision_value = top_1_precision(R_pred,test)\n",
    "        precision_at_k_value = precision_at_k(R_pred,test)\n",
    "        result = pd.DataFrame()\n",
    "        result['RMSE']=[rmse_value]\n",
    "        result['MAE']= [mae_value]\n",
    "        result['top_10_f1_score']= [top_10_f1_score_value]\n",
    "        result['top_10_precision']= [top_10_precision_value]\n",
    "        result['top_1_precision']= [top_1_precision_value]\n",
    "        result['precision_at_k']= [precision_at_k_value]\n",
    "        print(result)\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        filename = f\"output_{timestamp}.csv\"\n",
    "        result.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  0,  0, ...,  0,  0,  0],\n",
       "       [ 1,  0,  0, ...,  0,  0,  0],\n",
       "       [ 1,  0,  0, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [10,  0,  0, ...,  0,  0,  0],\n",
       "       [ 4,  0,  0, ...,  0,  0,  0],\n",
       "       [ 8,  0,  0, ...,  0,  0,  0]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_onehot.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan,  1., ...,  4., nan,  2.],\n",
       "       [nan, nan, nan, ..., nan, nan,  1.],\n",
       "       ...,\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "zxz = k_fold_cross_validation(rating_matrix.replace(to_replace=0, value=np.nan).values)\n",
    "train_m=zxz[0][0]\n",
    "test_m =zxz[0][1]\n",
    "train_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243\n"
     ]
    }
   ],
   "source": [
    "\n",
    "values = [0.001,  0.1,0]\n",
    "combinations = list(itertools.product(values, repeat=5))\n",
    "combinations_arr = np.array(combinations)\n",
    "selected_combinations = random.sample(combinations, 20)\n",
    "print(len(combinations_arr))\n",
    "model = NMF(n_components=10, init='random', random_state=0) #n_component = KNN values\n",
    "U_matrix = model.fit_transform(np.nan_to_num(train_m,0))\n",
    "V_matrix = model.components_\n",
    "filter_D = model.fit_transform(df_onehot.values)\n",
    "D_matrix = model.components_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.67540975e+00, -2.67540975e+00, -2.67540975e+00, -2.67540975e+00,\n",
       "       -1.15957761e+00, -1.15957761e+00, -1.15957761e+00, -1.15957761e+00,\n",
       "       -1.15957761e+00, -5.47732089e-01, -5.47732089e-01, -5.47732089e-01,\n",
       "       -5.47732089e-01, -5.47732089e-01, -2.84359053e-01, -2.84359053e-01,\n",
       "       -2.84359053e-01, -2.84359053e-01, -2.84359053e-01, -1.51871375e-01,\n",
       "       -1.51871375e-01, -1.51871375e-01, -1.51871375e-01, -1.51871375e-01,\n",
       "       -7.68442359e-02, -7.68442359e-02, -7.68442359e-02, -7.68442359e-02,\n",
       "       -7.68442359e-02, -3.05174061e-02, -3.05174061e-02, -3.05174061e-02,\n",
       "       -3.05174061e-02, -3.05174061e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        4.54437870e-03,  0.00000000e+00,  0.00000000e+00,  2.23332074e-02,\n",
       "        2.23332074e-02,  2.11305137e-02,  2.11305137e-02,  2.11305137e-02,\n",
       "        3.63499608e-02,  3.63499608e-02,  3.63499608e-02,  3.63499608e-02,\n",
       "        3.63499608e-02, -2.67540975e+00, -2.84359053e-01,  3.63499608e-02,\n",
       "       -1.51871375e-01, -2.84359053e-01, -2.84359053e-01,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  4.54437870e-03,\n",
       "        0.00000000e+00,  4.54437870e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "        1.51479290e-03,  1.51479290e-03,  3.02958580e-03,  3.02958580e-03,\n",
       "        0.00000000e+00,  3.02958580e-03,  3.02958580e-03,  0.00000000e+00,\n",
       "        0.00000000e+00,  1.51479290e-03,  3.02958580e-03,  0.00000000e+00,\n",
       "        3.02958580e-03,  0.00000000e+00,  1.51479290e-03,  0.00000000e+00,\n",
       "        0.00000000e+00,  3.02958580e-03,  3.02958580e-03,  3.02958580e-03,\n",
       "        3.02958580e-03,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  1.51479290e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        1.51479290e-03,  4.54437870e-03,  0.00000000e+00, -2.84359053e-01,\n",
       "       -2.84359053e-01,  0.00000000e+00,  3.02958580e-03,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -1.49504512e-01,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  4.54437870e-03,  0.00000000e+00,\n",
       "        0.00000000e+00,  3.02958580e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        1.51479290e-03,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  4.54437870e-03,  0.00000000e+00, -2.84359053e-01,\n",
       "        4.54437870e-03,  0.00000000e+00,  1.51479290e-03,  0.00000000e+00,\n",
       "        2.39509787e-03, -2.84359053e-01,  0.00000000e+00,  4.54437870e-03,\n",
       "        3.63499608e-02, -2.84359053e-01,  0.00000000e+00])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_i=np.array(calculate_L(similarity_matrix))/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 18/500 [00:18<07:45,  1.04it/s]"
     ]
    }
   ],
   "source": [
    "param_grid = {'alpha': [100,0.01],\n",
    "              'lambdas_': selected_combinations,\n",
    "              'R_': [train_m],\n",
    "              'P_': [0],\n",
    "              'U_':[U_matrix],\n",
    "              'V_':[V_matrix],\n",
    "              'Q_': [df_onehot.values],\n",
    "              'E_': [0],\n",
    "              'D_': [D_matrix],\n",
    "              'C_u_': [C_umatrix],\n",
    "              'C_i_': [C_imatrix],\n",
    "              'L_U_': [0],\n",
    "              'L_V_': [L_i],\n",
    "              'b_u_': [user_bias.values],\n",
    "              'b_v_': [item_bias.values]}\n",
    "best_params = grid_search(MultiMF,param_grid, test_m)\n",
    "alpha = best_params['alpha']\n",
    "lambdas = best_params['lambdas_']\n",
    "object = MultiMF(train_m,0,df_onehot.values,U_matrix,V_matrix,0,D_matrix,C_umatrix,C_imatrix,0,L_i,lambdas,user_bias.values,item_bias.values,alpha)\n",
    "object.evaluation(test_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:09<00:00,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 1 1 1]\n",
      " [1 1 2 ... 4 1 2]\n",
      " [2 2 2 ... 2 2 1]\n",
      " ...\n",
      " [1 1 2 ... 2 4 3]\n",
      " [1 2 2 ... 2 4 3]\n",
      " [2 2 2 ... 2 2 2]]\n",
      "       RMSE       MAE  top_10_f1_score  top_10_precision  top_1_precision  \\\n",
      "0  1.773828  1.459596         0.006387          0.021429          0.02381   \n",
      "\n",
      "   precision_at_k  \n",
      "0        0.021429  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# object = MultiMF(train_m,0,df_onehot.values,U_matrix,V_matrix,0,D_matrix,C_umatrix,C_imatrix,0,L_i,[0.1,0.01,0.1,0.001,0.01],user_bias.values,item_bias.values,0.001)\n",
    "# # object.factorize(5)\n",
    "# object.evaluation(test_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(object.total_loss[1:])\n",
    "5*3*200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd26f88697b863e66c6ece3fc4ab8542eed3593bce93b36f41fad6131c5cf6bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
